#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Text processing utilities for Youtubingest.

Includes functions for cleaning text, extracting URLs, formatting durations,
detecting language, and counting tokens.
"""

import functools
import html
import re
from typing import Any, List, Optional

import emoji
import isodate
import tiktoken
from langdetect import DetectorFactory, detect_langs, LangDetectException
from pathvalidate import sanitize_filename

# Import config for settings
from config import config
from logging_config import StructuredLogger

logger = StructuredLogger(__name__)

# Initialize language detector with a seed for consistent results
try:
    DetectorFactory.seed = 0
except Exception as e:
    logger.warning(f"Could not set langdetect seed: {e}")


@functools.lru_cache(maxsize=config.TEXT_CLEANING_CACHE_SIZE)
def extract_urls(text: str) -> List[str]:
    """Extract URLs from text content.

    Args:
        text: The text to extract URLs from

    Returns:
        list: List of URLs found in the text
    """
    if not text:
        return []

    try:
        # Improved regex to handle various URL schemes and avoid trailing punctuation
        url_pattern = re.compile(
            r"""
            (?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/) # Start of URL
            (?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+)          # URL path/query
            """,
            re.VERBOSE
        )
        matches = url_pattern.findall(text)

        # Clean up matches (findall returns tuples with capturing groups)
        cleaned_urls = []
        for match in matches:
            url = match[0] # The full match is the first element
            # Remove trailing punctuation that might be caught
            cleaned_url = re.sub(r'[.,;:!?\'")}\]\s]+$', '', url)
            cleaned_urls.append(cleaned_url)

        return cleaned_urls
    except Exception as e:
        logger.error(f"Error extracting URLs: {e}", error=str(e), exc_info=True)
        return []


@functools.lru_cache(maxsize=config.TEXT_CLEANING_CACHE_SIZE)
def clean_title(title: str) -> str:
    """Clean a video title by removing noise and formatting.

    Args:
        title: The title to clean

    Returns:
        str: Cleaned title
    """
    if not title:
        return "Unknown_Title"

    try:
        # Decode HTML entities first
        cleaned = html.unescape(title)

        # Remove emojis
        cleaned = emoji.replace_emoji(cleaned, replace="")

        # Define patterns to remove common noise
        patterns = [
            (re.compile(r"[\[({](?!\/)[^\])}]*[\])}]"), ""),  # Remove bracketed sections like [Official Video]
            (re.compile(r"#\w+"), ""),                       # Remove hashtags
            (re.compile(r"\b(?:official\s+video|music\s+video|lyric\s+video|audio|official|full\s+video|hd|4k)\b", re.I), ""), # Common suffixes/prefixes
            (re.compile(r"\s*\|\s*(?:[^|]+)$"), ""),          # Remove "| Channel Name" suffix if it looks like one
            (re.compile(r"^\s*[►▶→⟩⟬⟭▸▹◀◁►◄➞➔➤➧➨➪➮☛☞]?\s*"), ""), # Leading symbols/emojis
            (re.compile(r"(\s*-\s*Topic)$", re.I), ""),       # Remove "- Topic" suffix generated by YouTube
            (re.compile(r"\s+"), " ")                       # Normalize whitespace
        ]

        # Apply each pattern
        for pattern, replacement in patterns:
            cleaned = pattern.sub(replacement, cleaned)

        cleaned = cleaned.strip()

        # If cleaning resulted in empty string, return a placeholder
        if not cleaned:
            return "Cleaned_Title_Empty"

        # Sanitize filename-unsafe characters as a final step
        try:
            # Use pathvalidate for robust sanitization
            safe_cleaned = sanitize_filename(cleaned, platform="universal", replacement_text="_")
            # Ensure the result is not empty after sanitization
            return safe_cleaned if safe_cleaned else "Sanitized_Title_Empty"
        except Exception as e_sanitize:
            logger.debug(f"Pathvalidate sanitization failed: {e_sanitize}, using basic fallback")
            # Basic fallback sanitization
            safe_chars = re.sub(r'[\\/*?:"<>|]', "_", cleaned)
            safe_chars = re.sub(r'\s+', '_', safe_chars) # Replace spaces with underscores
            return safe_chars if safe_chars else "Sanitized_Title_Error"

    except Exception as e:
        logger.error(f"Error cleaning title '{title[:50]}...': {e}", title=title[:50], error=str(e), exc_info=True)
        # Fallback: return a sanitized version of the original title prefix
        try:
            return sanitize_filename(title[:50], platform="universal", replacement_text="_") if title else "Title_Cleaning_Error"
        except:
            return "Title_Cleaning_Error"


@functools.lru_cache(maxsize=config.TEXT_CLEANING_CACHE_SIZE)
def clean_description(text: str) -> str:
    """Clean description text by removing URLs, markdown and normalizing whitespace.

    Args:
        text: The description text to clean

    Returns:
        str: Cleaned description text
    """
    if not text:
        return ""

    try:
        # Truncate very long descriptions early to save processing
        if len(text) > 50000:
            text = text[:50000] + "... [truncated due to length]"
            logger.debug("Description truncated due to length > 50000")

        # Decode HTML entities
        text = html.unescape(text)

        # Remove URLs more aggressively
        text = re.sub(r"https?://\S+|www\.\S+", " ", text)

        # Remove markdown-like formatting (simplified)
        text = re.sub(r"[*_`~#]", "", text) # Remove common markdown chars
        text = re.sub(r"\[.*?\]\(.*?\)", " ", text) # Remove [links](...)

        # Remove emojis
        text = emoji.replace_emoji(text, replace="")

        # Normalize line breaks and whitespace
        text = text.replace("\r\n", "\n").replace("\r", "\n")
        text = re.sub(r"[ \t]+", " ", text)
        text = re.sub(r"\n{3,}", "\n\n", text) # Reduce multiple newlines

        # Remove non-printable characters (excluding newline and tab)
        text = "".join(char for char in text if char.isprintable() or char in "\n\t")

        # Remove common boilerplate lines/patterns (case-insensitive)
        boilerplate_patterns = [
            r"^\s*subscribe.*",
            r"^\s*follow me.*",
            r"^\s*check out.*",
            r"^\s*join .* discord.*",
            r"^\s*support .* patreon.*",
            r"^\s*merch.*",
            r"^\s*like,? comment,? and subscribe.*",
            r"^\s*don't forget to like.*",
            r"^\s*thanks for watching.*",
            r"^\s*music provided by.*",
            r"^\s*copyright ©.*",
            r"^\s*all rights reserved.*",
            r"^\s*#\w+.*", # Lines starting with hashtags
            r"^\s*-{3,}\s*$", # Lines with only hyphens
            r"^\s*={3,}\s*$", # Lines with only equals
            r"^\s*\*{3,}\s*$", # Lines with only asterisks
        ]
        lines = text.split('\n')
        cleaned_lines = []
        for line in lines:
            is_boilerplate = False
            for pattern in boilerplate_patterns:
                if re.match(pattern, line.strip(), re.IGNORECASE):
                    is_boilerplate = True
                    break
            if not is_boilerplate:
                cleaned_lines.append(line)
        text = "\n".join(cleaned_lines)

        return text.strip()
    except Exception as e:
        logger.error(f"Error cleaning description: {e}", error=str(e), exc_info=True)
        # Return the original text in case of error during cleaning
        return text


@functools.lru_cache(maxsize=config.TEXT_CLEANING_CACHE_SIZE)
def format_duration(duration_iso: str) -> str:
    """Format ISO duration to human-readable format (HH:MM:SS or MM:SS).

    Args:
        duration_iso: ISO 8601 duration string (e.g., 'PT1H2M3S')

    Returns:
        str: Formatted duration string (e.g., "1:02:03" or "12:34") or "Unknown Duration"
    """
    if not duration_iso:
        return "Unknown Duration"

    try:
        # Parse using isodate
        td = isodate.parse_duration(duration_iso)
        total_seconds = int(td.total_seconds())

        if total_seconds < 0:
            logger.warning(f"Invalid negative duration: {duration_iso}")
            return "Invalid Duration"

        hours, remainder = divmod(total_seconds, 3600)
        minutes, seconds = divmod(remainder, 60)

        if hours > 0:
            return f"{hours}:{minutes:02d}:{seconds:02d}"
        else:
            return f"{minutes}:{seconds:02d}"
    except (isodate.ISO8601Error, ValueError, TypeError) as e:
        logger.warning(f"Could not parse ISO duration '{duration_iso}': {e}. Trying fallback.", duration=duration_iso, error=str(e))
        # Attempt basic PT format parsing as fallback
        try:
            if duration_iso.startswith("PT"):
                h = m = s = 0
                time_part = duration_iso[2:]

                h_match = re.search(r'(\d+)H', time_part)
                if h_match: h = int(h_match.group(1))

                m_match = re.search(r'(\d+)M', time_part)
                if m_match: m = int(m_match.group(1))

                s_match = re.search(r'(\d+)S', time_part)
                if s_match: s = int(s_match.group(1))

                if h > 0:
                    return f"{h}:{m:02d}:{s:02d}"
                else:
                    return f"{m}:{s:02d}"
        except Exception as fallback_e:
            logger.warning(f"Fallback duration parsing failed for '{duration_iso}': {fallback_e}")
            pass # Fall through to return "Unknown Format"

        return "Unknown Format"


# Format timestamp without caching for unhashable types
def _format_timestamp_uncached(seconds: Any) -> str:
    """Format seconds to HH:MM:SS timestamp without caching.

    Internal helper function for _format_timestamp.

    Args:
        seconds: Number of seconds (can be int, float, or string representation)

    Returns:
        str: Formatted timestamp (e.g., "00:00:00")
    """
    # Default timestamp for invalid inputs
    default_timestamp = "00:00:00"

    # Handle non-numeric types that can't be converted
    if seconds is None:
        logger.warning("Error formatting timestamp: input is None")
        return default_timestamp

    # Handle boolean values (True=1, False=0)
    if isinstance(seconds, bool):
        # Always return 00:00:00 for boolean values
        return default_timestamp

    try:
        # Convert to float first, then to int (handles string representations)
        seconds_int = max(0, int(float(seconds)))
        hours, remainder = divmod(seconds_int, 3600)
        minutes, seconds_part = divmod(remainder, 60)

        # Always return HH:MM:SS format
        return f"{hours:02d}:{minutes:02d}:{seconds_part:02d}"
    except (ValueError, TypeError) as e:
        logger.warning(f"Error formatting timestamp for {seconds}s: {e}", seconds=seconds, error=str(e))
        return default_timestamp


# Cache heavily as timestamps repeat often in transcripts
@functools.lru_cache(maxsize=config.TEXT_CLEANING_CACHE_SIZE * 10)
def _format_timestamp(seconds: Any) -> str:
    """Format seconds to HH:MM:SS timestamp.

    Internal helper function, cached for performance.
    Uses a separate uncached function for unhashable types.

    Args:
        seconds: Number of seconds (can be int, float, or string representation)

    Returns:
        str: Formatted timestamp (e.g., "00:00:00")
    """
    # Handle unhashable types (dict, list, set) by using the uncached version
    if isinstance(seconds, (dict, list, set)):
        logger.warning(f"Error formatting timestamp: unhashable type {type(seconds).__name__}")
        return _format_timestamp_uncached(0)

    # For all other types, use the cached implementation
    return _format_timestamp_uncached(seconds)


def detect_language_with_fallback(text: str, default_code: str = "en") -> str:
    """Detect language with fallbacks.

    Uses langdetect library to identify the language of the text.

    Args:
        text: Text to detect language from
        default_code: Default language code to use if detection fails or text is too short

    Returns:
        str: Detected language code (e.g., 'en', 'fr')
    """
    if not text or len(text.strip()) < 20: # Need sufficient text for reliable detection
        return default_code

    try:
        # Use a sample for efficiency, especially for long texts
        sample = text[:1000]
        langs = detect_langs(sample)
        if langs:
            # Return the language code of the most probable language
            return langs[0].lang
        else:
            logger.debug("langdetect returned no results, using default.")
            return default_code
    except LangDetectException as e:
        logger.warning(f"Language detection failed: {e}. Using default '{default_code}'.")
        return default_code
    except Exception as e:
        logger.error(f"Unexpected error during language detection: {e}", exc_info=True)
        return default_code


# Cache the encoding to avoid reloading it for each token count
_tiktoken_encoding = None

def get_tiktoken_encoding(model_name: str = "gpt-3.5-turbo"):
    """Get or create a tiktoken encoding for token counting.

    Args:
        model_name: The name of the model to get the encoding for

    Returns:
        The tiktoken encoding for the specified model
    """
    global _tiktoken_encoding

    if _tiktoken_encoding is None:
        try:
            _tiktoken_encoding = tiktoken.encoding_for_model(model_name)
            logger.debug(f"Initialized tiktoken encoding for model: {model_name}")
        except Exception as e:
            logger.warning(f"Failed to load tiktoken encoding for {model_name}: {e}. Using cl100k_base fallback.")
            try:
                # Fallback to cl100k_base encoding (used by many GPT models)
                _tiktoken_encoding = tiktoken.get_encoding("cl100k_base")
            except Exception as e2:
                logger.error(f"Failed to load fallback encoding: {e2}", exc_info=True)
                return None

    return _tiktoken_encoding


def count_tokens(text: str, model_name: str = "gpt-3.5-turbo") -> Optional[int]:
    """Count the number of tokens in a text string using tiktoken.

    Args:
        text: The text to count tokens for
        model_name: The model name to use for token counting

    Returns:
        int: The number of tokens in the text, or None if counting failed
    """
    if not text:
        return 0

    try:
        encoding = get_tiktoken_encoding(model_name)
        if encoding is None:
            logger.warning("No tiktoken encoding available, token count unavailable")
            return None

        # Count tokens
        token_count = len(encoding.encode(text))
        return token_count
    except Exception as e:
        logger.error(f"Error counting tokens: {e}", exc_info=True)
        return None




