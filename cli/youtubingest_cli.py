#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
youtubingest_cli.py
Script pour l'extraction de m√©tadonn√©es et transcriptions de vid√©os YouTube.
Prend en charge les sorties aux formats TXT, Markdown et YAML.
Optimis√© pour un usage avec les intelligences artificielles conversationnelles (ou LLM).
"""

from __future__ import annotations # Pour des annotations de type plus propres dans les classes

# Imports de la biblioth√®que standard
import asyncio
import functools
import logging
import os
import random
import re
import sys
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import (Any, AsyncIterator, Callable, Dict, List, Optional, Set,
                    Tuple, Union, Coroutine) # Ajout de Coroutine
from urllib.parse import parse_qs, unquote_plus, urlparse

# Imports de biblioth√®ques tierces
import emoji
import isodate
import yaml # Ajout√© pour la sortie YAML
import tiktoken
from googleapiclient.discovery import Resource, build
from googleapiclient.errors import HttpError
from pathvalidate import sanitize_filename
from rich.console import Console, Group, RenderableType
from rich.live import Live
from rich.prompt import Confirm, Prompt
from rich.table import Table
from rich.text import Text
from youtube_transcript_api import (CouldNotRetrieveTranscript, NoTranscriptFound,
                                    Transcript, TranscriptsDisabled,
                                    YouTubeTranscriptApi)
from youtube_transcript_api import \
    _errors as YouTubeTranscriptApiErrors # Pour l'acc√®s aux erreurs de fetch

# ==============================================================================
# SECTION 1 : Configuration et Journalisation (Logging)
# ==============================================================================

@dataclass
class Config:
    """Contient les param√®tres de configuration pour le scraper."""
    API_KEY: str = os.environ.get("YOUTUBE_API_KEY", "") # Cl√© API YouTube (via variable d'environnement)
    WORK_FOLDER: Path = Path(__file__).parent.resolve() # Dossier de travail du script
    VIDEOS_FOLDER: Path = WORK_FOLDER / "Vid√©os" # Sous-dossier pour les fichiers de sortie
    YOUTUBE_BASE_URL: str = "https://www.youtube.com" # URL de base de YouTube
    BATCH_SIZE: int = 50 # Nombre max d'√©l√©ments par requ√™te API (videos/playlistItems)
    MAX_TOKENS_PER_FILE: int = 100000 # Limite approx. de tokens par fichier de sortie (0 = pas de limite)
    TRANSCRIPT_LANGUAGES: Tuple[str, ...] = ("fr", "en", "es", "pt", "it", "de") # Langues de transcription pr√©f√©r√©es (ordre de priorit√©)
    MIN_DURATION: timedelta = timedelta(seconds=20) # Dur√©e minimale des vid√©os √† traiter
    MIN_DELAY: int = 150   # D√©lai minimal (ms) entre les appels API
    MAX_DELAY: int = 600   # D√©lai maximal (ms) entre les appels API
    LIMIT_DATE: datetime = datetime(2020, 1, 1, tzinfo=timezone.utc) # Ignorer les vid√©os publi√©es avant cette date
    MAX_SEARCH_RESULTS: int = 200 # Nombre max de vid√©os √† r√©cup√©rer pour une recherche
    TRANSCRIPT_BLOCK_DURATION_SECONDS: int = 10 # Regrouper les lignes de transcription par blocs de cette dur√©e
    RESOLVE_CACHE_SIZE: int = 128 # Taille du cache LRU pour la r√©solution d'ID de cha√Æne
    TRANSCRIPT_CACHE_SIZE: int = 512 # Taille du cache LRU pour le listage/r√©cup√©ration de transcriptions
    URL_PARSE_CACHE_SIZE: int = 256 # Taille du cache LRU pour l'analyse d'URL
    PLAYLIST_ITEM_CACHE_SIZE: int = 64 # Taille du cache manuel (FIFO) pour les pages d'items de playlist
    TRANSCRIPT_SEMAPHORE_LIMIT: int = 10 # Limite de requ√™tes concurrentes √† l'API de transcription

config = Config()

# S'assurer que les dossiers de travail existent
config.WORK_FOLDER.mkdir(parents=True, exist_ok=True)
config.VIDEOS_FOLDER.mkdir(parents=True, exist_ok=True)

# Configuration de la journalisation (logging)
logging.basicConfig(
    level=logging.INFO, # Niveau de log par d√©faut
    format="%(asctime)s [%(levelname)s] %(threadName)s %(message)s | %(funcName)s:%(lineno)d", # Format des messages
    handlers=[logging.FileHandler(config.WORK_FOLDER / "youtubingest_cli.log", mode="w", encoding="utf-8")], # √âcriture dans un fichier
    datefmt='%Y-%m-%d %H:%M:%S' # Format de la date
)
logger = logging.getLogger(__name__)

# Ajout d'un niveau de log personnalis√© "SPAM" pour les d√©tails tr√®s fins
SPAM_LEVEL_NUM = 5
logging.addLevelName(SPAM_LEVEL_NUM, "SPAM")
def spam(self, message, *args, **kws):
    """M√©thode pour logger au niveau SPAM."""
    # V√©rifie si le logger est activ√© pour ce niveau avant de logger
    if self.isEnabledFor(SPAM_LEVEL_NUM):
        self._log(SPAM_LEVEL_NUM, message, args, **kws)
logging.Logger.spam = spam
# D√©commenter la ligne suivante pour activer les logs de niveau SPAM
# logging.getLogger().setLevel(SPAM_LEVEL_NUM)

class QuotaExceededError(Exception):
    """Exception personnalis√©e lev√©e lorsque le quota de l'API YouTube est d√©pass√©."""
    pass

# ==============================================================================
# SECTION 2 : Mod√®les de Donn√©es (Data Models)
# ==============================================================================

@dataclass
class Video:
    """Repr√©sente une vid√©o YouTube avec ses m√©tadonn√©es et sa transcription."""
    id: str # ID unique de la vid√©o YouTube
    snippet: Dict[str, Any] = field(default_factory=dict) # Donn√©es du 'snippet' de l'API
    contentDetails: Dict[str, Any] = field(default_factory=dict) # Donn√©es 'contentDetails' de l'API
    transcript: Optional[Dict[str, str]] = field(default=None, repr=False) # Transcription trait√©e: {"language": "xx", "transcript": "..."}
    tags: List[str] = field(default_factory=list) # Liste des tags associ√©s √† la vid√©o
    description_urls: List[str] = field(default_factory=list) # URLs extraites de la description
    video_transcript_language: Optional[str] = None # Code langue r√©el de la transcription r√©cup√©r√©e

    @classmethod
    def from_api_response(cls, item: Dict[str, Any]) -> Video:
        """
        Cr√©e une instance de Video √† partir d'un √©l√©ment de r√©ponse de l'API YouTube (videos.list).

        Args:
            item: Le dictionnaire repr√©sentant la ressource vid√©o de l'API.

        Returns:
            Une instance de la classe Video.
        """
        snippet = item.get('snippet', {})
        description = snippet.get('description', "")
        return cls(
            id=item.get('id', 'unknown_id'), # Fournir une valeur par d√©faut si l'ID manque
            snippet=snippet,
            contentDetails=item.get('contentDetails', {}),
            tags=snippet.get('tags', []),
            description_urls=extract_urls(description)
        )

    # --- Propri√©t√©s pour un acc√®s facile aux donn√©es ---
    @property
    def title(self) -> str:
        """Retourne le titre de la vid√©o."""
        return self.snippet.get("title", "")
    @property
    def description(self) -> str:
        """Retourne la description brute de la vid√©o."""
        return self.snippet.get("description", "")
    @property
    def channel_id(self) -> str:
        """Retourne l'ID de la cha√Æne YouTube."""
        return self.snippet.get("channelId", "")
    @property
    def channel_title(self) -> str:
        """Retourne le nom de la cha√Æne YouTube."""
        return self.snippet.get("channelTitle", "")
    @property
    def url(self) -> str:
        """Retourne l'URL compl√®te de la vid√©o."""
        return f"{config.YOUTUBE_BASE_URL}/watch?v={self.id}"
    @property
    def channel_url(self) -> str:
        """Retourne l'URL de la cha√Æne, si l'ID est disponible."""
        return f"{config.YOUTUBE_BASE_URL}/channel/{self.channel_id}" if self.channel_id else ""
    @property
    def duration_iso(self) -> str:
        """Retourne la dur√©e au format ISO 8601 (ex: 'PT1M30S')."""
        return self.contentDetails.get("duration", "")
    @property
    def published_at_iso(self) -> str:
        """Retourne la date de publication au format ISO 8601 (cha√Æne)."""
        return self.snippet.get("publishedAt", "")
    @property
    def default_language(self) -> Optional[str]:
        """Retourne la langue par d√©faut des m√©tadonn√©es (ex: 'en-US')."""
        return self.snippet.get("defaultLanguage")
    @property
    def default_audio_language(self) -> Optional[str]:
        """Retourne la langue audio par d√©faut (ex: 'en')."""
        return self.snippet.get("defaultAudioLanguage")

    # --- M√©thodes utilitaires ---
    def get_published_at_datetime(self) -> Optional[datetime]:
        """
        Analyse la cha√Æne publishedAt ISO 8601 en un objet datetime conscient du fuseau horaire (UTC).

        Returns:
            Un objet datetime ou None si l'analyse √©choue.
        """
        if not self.published_at_iso: return None
        try:
            ts = self.published_at_iso
            # S'assurer que le format est bien UTC (RFC3339)
            if not ts.endswith('Z'): ts += 'Z'
            # Utiliser strptime comme dans la version originale pour compatibilit√©
            return datetime.strptime(ts, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc)
        except (ValueError, TypeError):
            logger.warning(f"Format de date de publication invalide pour la vid√©o {self.id}: {self.published_at_iso}")
            return None

    def get_duration_seconds(self) -> Optional[int]:
        """
        Analyse la cha√Æne de dur√©e ISO 8601 en nombre total de secondes.

        Returns:
            Le nombre total de secondes (entier) ou None si l'analyse √©choue.
        """
        if not self.duration_iso: return None
        try:
            td: timedelta = isodate.parse_duration(self.duration_iso)
            seconds = int(td.total_seconds())
            return seconds if seconds >= 0 else None # Retourner None pour dur√©es n√©gatives
        except (isodate.ISO8601Error, ValueError, TypeError):
            logger.warning(f"Format de dur√©e invalide pour la vid√©o {self.id}: {self.duration_iso}")
            return None

    # --- M√©thodes de formatage de sortie ---
    def to_texte(self, include_description: bool = True) -> str:
        """
        Formate les donn√©es de la vid√©o en un bloc de texte brut.

        Args:
            include_description: Inclure la description nettoy√©e dans la sortie.

        Returns:
            Une cha√Æne de caract√®res format√©e en texte brut.
        """
        published_at_dt_utc = self.get_published_at_datetime()
        published_at_str = published_at_dt_utc.strftime("%Y-%m-%d %H:%M:%S (UTC)") if published_at_dt_utc else "Date inconnue"
        formatted_duration = format_duration(self.duration_iso)
        tags_str = ", ".join(f"'{tag}'" for tag in self.tags) if self.tags else "Aucun"
        cleaned_title = clean_title(self.title)
        cleaned_description = clean_description(self.description.strip()) if include_description and self.description else ""
        transcription_section = self._get_transcription_section_text() # Utilise l'helper interne

        metadata_lines = [
            f"- Date de publication : {published_at_str}",
            f"- Dur√©e : {formatted_duration}",
            f"- Tags : {tags_str}",
            f"- URL vid√©o : <{self.url}>",
            f"- Nom cha√Æne : {self.channel_title}",
        ]
        if self.channel_url:
            metadata_lines.append(f"- URL cha√Æne : <{self.channel_url}>")

        texte = f"Titre vid√©o : {cleaned_title}\n\nM√©tadonn√©es :\n" + "\n".join(metadata_lines) + "\n\n"
        if include_description and cleaned_description:
            texte += f"Description :\n{cleaned_description}\n\n"
        texte += transcription_section
        return texte.strip()

    def to_markdown(self, include_description: bool = True) -> str:
        """
        Formate les donn√©es de la vid√©o en une section Markdown.

        Args:
            include_description: Inclure la description nettoy√©e dans la sortie.

        Returns:
            Une cha√Æne de caract√®res format√©e en Markdown.
        """
        published_at_dt_utc = self.get_published_at_datetime()
        published_at_str = published_at_dt_utc.strftime("%Y-%m-%d %H:%M:%S (UTC)") if published_at_dt_utc else "Date inconnue"
        formatted_duration = format_duration(self.duration_iso)
        # Formater les tags comme du code inline en Markdown
        tags_str = ", ".join(f"`{tag}`" for tag in self.tags) if self.tags else "Aucun"
        cleaned_title = clean_title(self.title)
        cleaned_description = clean_description(self.description.strip()) if include_description and self.description else ""
        transcript_lang, transcript_text = self._get_transcript_lang_and_text() # Utilise l'helper interne

        md_lines = [f"## {cleaned_title}"] # Titre de niveau 2
        md_lines.append("\n### M√©tadonn√©es") # Sous-section pour les m√©tadonn√©es
        md_lines.append(f"*   **Date de publication**: {published_at_str}")
        md_lines.append(f"*   **Dur√©e**: {formatted_duration}")
        md_lines.append(f"*   **Tags**: {tags_str}")
        md_lines.append(f"*   **URL vid√©o**: <{self.url}>")
        md_lines.append(f"*   **Nom cha√Æne**: {self.channel_title}")
        if self.channel_url:
            md_lines.append(f"*   **URL cha√Æne**: <{self.channel_url}>")

        if include_description and cleaned_description:
            md_lines.append("\n### Description") # Sous-section pour la description
            # Ajouter des sauts de ligne avant/apr√®s pour un meilleur rendu
            md_lines.append(f"\n{cleaned_description}\n")

        if transcript_text:
            md_lines.append(f"\n### Transcription (Langue: `{transcript_lang}`)") # Sous-section transcription
            # Utiliser un bloc de code 'text' pour la transcription
            md_lines.append(f"\n```text\n{transcript_text}\n```\n")
        else:
            md_lines.append("\n### Transcription")
            md_lines.append("\n*Aucune transcription disponible/trait√©e.*")

        return "\n".join(md_lines).strip()

    def to_dict(self, include_description: bool = True) -> Dict[str, Any]:
        """
        Retourne une repr√©sentation dictionnaire des donn√©es de la vid√©o,
        adapt√©e pour la s√©rialisation (ex: YAML, JSON).

        Args:
            include_description: Inclure la description nettoy√©e dans le dictionnaire.

        Returns:
            Un dictionnaire contenant les donn√©es cl√©s de la vid√©o.
        """
        published_at_dt = self.get_published_at_datetime()
        transcript_lang, transcript_text = self._get_transcript_lang_and_text() # Utilise l'helper interne

        data: Dict[str, Any] = {
            "id": self.id,
            "title": clean_title(self.title),
            "url": self.url,
            "published_at_iso": self.published_at_iso if self.published_at_iso else None,
            "published_at_utc": published_at_dt.isoformat() if published_at_dt else None, # Format ISO standard pour datetime
            "duration_iso": self.duration_iso if self.duration_iso else None,
            "duration_seconds": self.get_duration_seconds(), # Inclure la dur√©e en secondes
            "channel_name": self.channel_title,
            "channel_id": self.channel_id,
            "channel_url": self.channel_url if self.channel_url else None,
            "tags": self.tags if self.tags else [], # Assurer une liste vide si pas de tags
            "description_urls": self.description_urls if self.description_urls else [], # Assurer une liste vide
        }
        if include_description:
            # Ajouter la description seulement si demand√©e et si elle existe
            cleaned_desc = clean_description(self.description.strip()) if self.description else None
            data["description"] = cleaned_desc

        # Ajouter les informations de transcription
        data["transcript_language"] = transcript_lang # Sera None si pas de transcript
        data["transcript_text"] = transcript_text # Sera None si pas de transcript

        # Optionnel : Nettoyer les cl√©s ayant une valeur None pour un YAML/JSON plus concis
        return {k: v for k, v in data.items() if v is not None}

    # --- Helpers internes pour le formatage ---
    def _get_transcription_section_text(self) -> str:
        """Helper interne pour obtenir la section transcription format√©e pour la sortie TXT."""
        lang, text = self._get_transcript_lang_and_text()
        if text:
            return f"Transcription (langue {lang}) :\n{text}\n"
        return "Transcription : Aucune transcription disponible/trait√©e.\n"

    def _get_transcript_lang_and_text(self) -> Tuple[Optional[str], Optional[str]]:
        """Helper interne pour r√©cup√©rer de mani√®re s√ªre la langue et le texte de la transcription."""
        if self.transcript and isinstance(self.transcript, dict):
            # Utilise la langue stock√©e lors du fetch, sinon celle du dict transcript
            lang = self.video_transcript_language or self.transcript.get('language')
            text = self.transcript.get('transcript')
            return lang, text
        return None, None # Retourne None pour les deux si pas de transcript

# ==============================================================================
# SECTION 3 : Utilitaires de Traitement de Texte
# ==============================================================================

@functools.lru_cache(maxsize=1024)
def extract_urls(text: str) -> List[str]:
    """
    Extrait toutes les URLs d'une cha√Æne de caract√®res donn√©e.

    Args:
        text: La cha√Æne de caract√®res √† analyser.

    Returns:
        Une liste des URLs trouv√©es.
    """
    if not text: return []
    try:
        # Regex robuste pour capturer les URLs, g√©rant les parenth√®ses et caract√®res de fin courants
        urls = re.findall(r'https?://[^\s<>"\')]+(?:\([^\s<>"]*\)|[^\s<>"\')`])', text)
        # Nettoyer la ponctuation finale des URLs captur√©es
        return [url.rstrip('.,;)!?]\'"') for url in urls]
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction des URLs: {e}")
        return []

@functools.lru_cache(maxsize=512)
def clean_title(title: str) -> str:
    """
    Nettoie un titre de vid√©o en supprimant les √©l√©ments superflus courants
    et en le rendant s√ªr pour une utilisation comme nom de fichier.

    Args:
        title: Le titre original de la vid√©o.

    Returns:
        Le titre nettoy√©.
    """
    if not title: return "Titre_Inconnu"
    try:
        cleaned = emoji.replace_emoji(title, replace='') # Supprimer les emojis

        # Regex am√©lior√©e pour supprimer les motifs courants :
        # - Texte entre crochets/parenth√®ses : [info], (info)
        # - Hashtags : #motcl√©
        # - Indicateurs pub/sponsor : ad:, pub:, sponsor(ed): ...jusqu'√† la fin
        # - Marqueurs courants : *NEW*, !LIVE!, X watching now
        # - S√©parateurs pipe et texte suivant : | Suite du titre
        # - Symboles 'play' en d√©but de ligne : ‚ñ∫
        cleaned = re.sub(
            r'[\[\(].*?[\]\)]|#\w+|\b(ad|pub|sponsor(ed)?)\b:?.*$|\*?NEW\*?|\!?LIVE\!?|\d+\s+watching now|\|.*$|^\s*‚ñ∫\s*',
            '', cleaned, flags=re.I | re.UNICODE
        )

        # Rendre s√ªr pour le syst√®me de fichiers (multi-plateforme)
        cleaned = sanitize_filename(cleaned.strip(), platform="universal")
        # Normaliser les espaces multiples en un seul espace
        cleaned = ' '.join(cleaned.split()).strip()
        # Retourner un titre par d√©faut si le nettoyage r√©sulte en une cha√Æne vide
        return cleaned if cleaned else "Titre_Nettoye_Vide"
    except Exception as e:
        logger.error(f"Erreur lors du nettoyage du titre '{title}': {e}")
        # Solution de repli : nettoyage de base du nom de fichier
        safe_fallback = sanitize_filename(title, platform="universal") if title else "Titre_Erreur_Nettoyage"
        return safe_fallback if safe_fallback else "Titre_Erreur_Nettoyage"

@functools.lru_cache(maxsize=512)
def clean_description(text: str) -> str:
    """
    Nettoie le texte de description en supprimant le HTML, les URLs, les emojis,
    les caract√®res de formatage Markdown et les espaces excessifs.

    Args:
        text: La description originale.

    Returns:
        La description nettoy√©e.
    """
    if not text: return ""
    try:
        text = re.sub(r'<[^>]+>', ' ', text) # Supprimer les balises HTML
        text = emoji.replace_emoji(text, replace='') # Supprimer les emojis

        # Supprimer les images et liens Markdown de mani√®re plus fiable
        text = re.sub(r'!\[.*?\]\(https?://\S+\)', '', text) # Images: ![alt](url)
        text = re.sub(r'\[(.*?)\]\(https?://\S+\)', r'\1', text) # Liens: [texte](url) -> texte

        text = re.sub(r'https?://\S+', '', text) # Supprimer les URLs restantes
        # Supprimer les caract√®res sp√©ciaux/Markdown courants
        text = re.sub(r'[\`*~\[\]{}<>|#_]', '', text)

        # Normaliser les sauts de ligne et les espaces
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        text = re.sub(r'[ \t]+', ' ', text) # Remplacer espaces/tabs multiples par un seul espace
        text = re.sub(r'\n{3,}', '\n\n', text) # R√©duire les lignes vides multiples √† deux max

        # Conserver les caract√®res imprimables + newline/tab, supprimer les autres.
        # ATTENTION : Peut supprimer des caract√®res l√©gitimes non-latins ou symboles.
        # √âvaluer si c'est trop agressif pour le LLM cible.
        text = ''.join(char for char in text if char.isprintable() or char in '\n\t')

        return text.strip()
    except Exception as e:
        logger.error(f"Erreur lors du nettoyage de la description: {e}")
        return text # Retourner le texte original en cas d'erreur

@functools.lru_cache(maxsize=256)
def format_duration(duration_iso: str) -> str:
    """
    Formate une cha√Æne de dur√©e ISO 8601 (ex: 'PT1M30S') en HH:MM:SS ou MM:SS.

    Args:
        duration_iso: La cha√Æne de dur√©e au format ISO 8601.

    Returns:
        La dur√©e format√©e ou une cha√Æne indiquant une erreur/inconnue.
    """
    if not duration_iso: return "Dur√©e inconnue"
    try:
        td: timedelta = isodate.parse_duration(duration_iso)
        total_seconds = int(td.total_seconds())
        if total_seconds < 0: return "Dur√©e invalide" # G√©rer les dur√©es n√©gatives

        hours, remainder = divmod(total_seconds, 3600)
        minutes, seconds = divmod(remainder, 60)

        # Afficher les heures seulement si elles sont > 0
        if hours > 0:
            return f"{hours}:{minutes:02d}:{seconds:02d}"
        else:
            return f"{minutes}:{seconds:02d}"
    except (isodate.ISO8601Error, ValueError, TypeError):
        logger.warning(f"Format de dur√©e invalide : {duration_iso}")
        return duration_iso # Retourner l'original en cas d'erreur

@functools.lru_cache(maxsize=1024)
def _format_timestamp(seconds: float) -> str:
    """
    Formate un nombre de secondes en une cha√Æne de caract√®res HH:MM:SS.

    Args:
        seconds: Le nombre de secondes (peut √™tre flottant).

    Returns:
        Le timestamp format√©.
    """
    try:
        if seconds < 0: seconds = 0.0 # Assurer que les secondes ne sont pas n√©gatives
        total_seconds_int = int(seconds)
        hours, remainder = divmod(total_seconds_int, 3600)
        minutes, seconds_part = divmod(remainder, 60)
        # Toujours retourner HH:MM:SS pour la coh√©rence dans les transcriptions
        return f"{hours:02d}:{minutes:02d}:{seconds_part:02d}"
    except Exception as e:
        logger.warning(f"Erreur lors du formatage du timestamp pour {seconds}s: {e}")
        return "??:??:??" # Timestamp d'erreur

# ==============================================================================
# SECTION 4 : Affichage de la Progression CLI
# ==============================================================================

class ProgressStatus:
    """Constantes simples pour les statuts d'affichage de la progression."""
    PENDING = "pending"; IN_PROGRESS = "in_progress"; DONE = "done"; ERROR = "error"
    INFO = "info"; WARNING = "warning"; SORT = "sort"; QUOTA = "quota"; SKIPPED = "skipped"

class ProgressDisplay:
    """G√®re l'affichage en direct de la progression dans la console en utilisant Rich."""
    # Mapping des statuts aux ic√¥nes emoji
    ICONS: Dict[str, str] = {
        ProgressStatus.PENDING: "üîò", ProgressStatus.IN_PROGRESS: "‚è≥", ProgressStatus.DONE: "üü¢",
        ProgressStatus.ERROR: "‚ùå", ProgressStatus.INFO: "‚ÑπÔ∏è", ProgressStatus.WARNING: "‚ö†Ô∏è",
        ProgressStatus.SORT: "‚áÖ", ProgressStatus.QUOTA: "üö´", ProgressStatus.SKIPPED: "‚ö™"
    }
    # Mapping des statuts aux styles Rich
    STYLE_MAP: Dict[str, str] = {
        ProgressStatus.IN_PROGRESS: "yellow", ProgressStatus.DONE: "green", ProgressStatus.ERROR: "red",
        ProgressStatus.WARNING: "yellow", ProgressStatus.SORT: "cyan", ProgressStatus.QUOTA: "bold red",
        ProgressStatus.SKIPPED: "dim", ProgressStatus.PENDING: "dim", ProgressStatus.INFO: "blue"
    }
    # Mapping des types de message aux pr√©fixes pour les messages g√©n√©raux
    PREFIX_MAP: Dict[str, str] = {
        ProgressStatus.ERROR: f"{ICONS[ProgressStatus.ERROR]} ERREUR: ",
        ProgressStatus.WARNING: f"{ICONS[ProgressStatus.WARNING]} ATTENTION: ",
        ProgressStatus.QUOTA: f"{ICONS[ProgressStatus.QUOTA]} QUOTA D√âPASS√â: ",
        ProgressStatus.SKIPPED: f"{ICONS[ProgressStatus.SKIPPED]} IGNOR√â: ",
        ProgressStatus.DONE: f"{ICONS[ProgressStatus.DONE]} SUCC√àS: ",
        ProgressStatus.INFO: f"    {ICONS[ProgressStatus.INFO]} INFO: " # Indentation pour les infos
    }

    def __init__(self):
        """Initialise le ProgressDisplay."""
        self.console: Console = Console(stderr=True, highlight=False) # Sortie sur stderr pour l'UI
        self.total_urls: int = 0 # Nombre total d'URLs √† traiter
        self.output_lines: List[RenderableType] = [] # Lignes √† afficher dans le Live display
        # Structure: {url_index: {step_number: {'line_index': int, 'step_name': str}}}
        self.url_steps_mapping: Dict[int, Dict[int, Dict[str, Any]]] = {}
        self.url_counter: int = 0 # Compteur pour assigner un index unique √† chaque URL trait√©e
        self._lock: asyncio.Lock = asyncio.Lock() # Verrou pour prot√©ger l'acc√®s concurrent aux donn√©es partag√©es
        self.live: Optional[Live] = None # Instance Rich Live pour l'affichage dynamique

    def show_header(self) -> None:
        """Affiche l'en-t√™te initial du script."""
        self.console.print("\n[bold blue]YouTubingest CLI[/]")
        self.console.print("[dim]Extraction m√©tadonn√©es, transcriptions, tags YouTube[/]\n")

    def show_menu(self) -> Tuple[str, str, str, bool, bool]:
        """
        Affiche le menu d'entr√©e et demande les options √† l'utilisateur.

        Returns:
            Un tuple contenant: (mode_entree, valeur_entree, format_sortie, inclure_transcription, inclure_description).
        """
        self.console.print("[bold]Options d'entr√©e :[/]")
        self.console.print("1. URL YouTube (Cha√Æne, Vid√©o, Playlist, Recherche)")
        self.console.print("2. Fichier d'URLs")
        mode = Prompt.ask("[blue]Mode[/]", choices=["1", "2"], default="1", console=self.console)

        default_file = "URL_dev.txt" # Fichier par d√©faut sugg√©r√© en mode 2
        prompt_text = "[blue]URL YouTube[/]" if mode == "1" else "[blue]Chemin Fichier[/]"
        default_value = default_file if mode == '2' and Path(default_file).exists() else None
        input_val = Prompt.ask(prompt_text, default=default_value, console=self.console)

        # --- Nouveau Prompt pour le Format de Sortie ---
        self.console.print("\n[bold]Options de sortie :[/]")
        output_format = Prompt.ask(
            "[blue]Format de sortie[/]",
            choices=["txt", "md", "yaml"], # Options disponibles
            default="txt", # Format par d√©faut
            console=self.console
        )
        # --- Fin Nouveau Prompt ---

        inc_transcript = Confirm.ask("\n[blue]Inclure transcriptions ?[/]", default=True, console=self.console)
        inc_desc = Confirm.ask("[blue]Inclure descriptions ?[/]", default=True, console=self.console)
        self.console.print("-" * 30) # S√©parateur visuel
        # Retourne toutes les options choisies
        return mode, input_val or "", output_format, inc_transcript, inc_desc

    async def start_processing(self, total_urls: int) -> None:
        """Initialise ou r√©initialise l'affichage Live pour une nouvelle s√©rie de traitements."""
        async with self._lock: # Assurer l'atomicit√© de la r√©initialisation
            self.total_urls = total_urls
            self.output_lines, self.url_steps_mapping, self.url_counter = [], {}, 0
            if self.live is None:
                 # Cr√©er l'instance Live si elle n'existe pas
                 self.live = Live("", refresh_per_second=4, console=self.console, transient=True, vertical_overflow="visible")
            try:
                if not self.live.is_started:
                    self.live.start(refresh=True) # D√©marrer le Live display
                else:
                    # Si d√©j√† d√©marr√© (ex: run pr√©c√©dent), effacer l'ancien contenu
                    self.live.update(Group(""), refresh=True)
                logger.debug(f"Affichage Live d√©marr√©/r√©initialis√© pour {total_urls} URLs.")
            except Exception as e:
                logger.error(f"Erreur lors du d√©marrage/r√©initialisation de l'affichage Live: {e}", exc_info=True)

    async def show_url_header(self, url: str) -> int:
        """
        Ajoute l'en-t√™te pour le traitement d'une URL sp√©cifique √† l'affichage Live.

        Args:
            url: L'URL en cours de traitement.

        Returns:
            L'index assign√© √† cette URL pour les mises √† jour futures.
        """
        async with self._lock:
            self.url_counter += 1
            url_index = self.url_counter # Index unique pour cette URL
            # Ajouter la ligne d'en-t√™te de l'URL
            self.output_lines.append(Text(f"\nTraitement URL {url_index}/{self.total_urls} : {url}\n", style="bold blue"))
            # D√©finir les √©tapes pour cette URL
            steps = ["Analyse URL", "R√©cup IDs Vid√©os", "R√©cup D√©tails/Transcr.", "Tri Vid√©os", "Sauvegarde"]
            self.url_steps_mapping[url_index] = {}
            # Ajouter une ligne pour chaque √©tape avec le statut initial 'pending'
            for i, step_name in enumerate(steps, 1):
                line = Text(f"  √âtape {i}/{len(steps)} : {self.ICONS[ProgressStatus.PENDING]} {step_name}", style=self.STYLE_MAP[ProgressStatus.PENDING])
                self.output_lines.append(line)
                # Stocker l'index de la ligne et le nom de l'√©tape pour les mises √† jour futures
                self.url_steps_mapping[url_index][i] = {'line_index': len(self.output_lines) - 1, 'step_name': step_name}
            # Mettre √† jour l'affichage Live
            await self._update_live_display_unsafe()
            return url_index # Retourner l'index assign√©

    async def update_step(self, url_index: int, step_number: int, status: str, progress_percent: Optional[float] = None, message: Optional[str] = None) -> None:
        """
        Met √† jour le statut d'une √©tape sp√©cifique pour une URL dans l'affichage Live.

        Args:
            url_index: L'index de l'URL (retourn√© par show_url_header).
            step_number: Le num√©ro de l'√©tape (1-based).
            status: Le nouveau statut (ex: ProgressStatus.IN_PROGRESS).
            progress_percent: Pourcentage de progression (pour statut IN_PROGRESS).
            message: Message additionnel √† afficher pour l'√©tape.
        """
        async with self._lock:
            # V√©rifier si l'URL et l'√©tape existent dans notre mapping
            if url_index not in self.url_steps_mapping or step_number not in self.url_steps_mapping[url_index]:
                logger.warning(f"Tentative de mise √† jour d'une √©tape inexistante : URL {url_index}, √âtape {step_number}")
                return
            try:
                step_info = self.url_steps_mapping[url_index][step_number]
                line_index, step_name = step_info['line_index'], step_info['step_name']
                num_steps = len(self.url_steps_mapping[url_index])

                # V√©rifier la validit√© de l'index de ligne
                if not (0 <= line_index < len(self.output_lines)):
                    logger.error(f"Index de ligne invalide ({line_index}) pour √©tape {step_number}, URL {url_index}")
                    return

                # Construire la nouvelle ligne de texte pour l'√©tape
                emoji_icon = self.ICONS.get(status, self.ICONS[ProgressStatus.INFO]) # Ic√¥ne par d√©faut INFO
                progress = f" ({progress_percent:.0f}%)" if status == ProgressStatus.IN_PROGRESS and progress_percent is not None else ""
                style = self.STYLE_MAP.get(status, self.STYLE_MAP[ProgressStatus.PENDING]) # Style par d√©faut PENDING
                display_msg = ""
                if message:
                    # Tronquer les messages longs pour l'affichage
                    truncated_msg = (message[:70] + '‚Ä¶') if len(message) > 70 else message
                    display_msg = f" ({truncated_msg})"

                # Mettre √† jour la ligne correspondante dans la liste des lignes √† afficher
                self.output_lines[line_index] = Text(f"  √âtape {step_number}/{num_steps} : {emoji_icon} {step_name}{progress}{display_msg}", style=style)
                # Rafra√Æchir l'affichage Live
                await self._update_live_display_unsafe()
            except Exception as e:
                logger.error(f"Erreur lors de la mise √† jour de l'√©tape {step_number} (URL {url_index}): {e}", exc_info=True)

    async def show_message_in_live(self, message_type: str, message: str) -> None:
        """Ajoute un message g√©n√©ral (info, avertissement, erreur) √† l'affichage Live."""
        async with self._lock:
            style = self.STYLE_MAP.get(message_type, self.STYLE_MAP[ProgressStatus.INFO])
            prefix = self.PREFIX_MAP.get(message_type, self.PREFIX_MAP[ProgressStatus.INFO])
            # Ajouter le message format√© √† la fin de la liste des lignes
            self.output_lines.append(Text(f"{prefix}{message}", style=style))
            # Rafra√Æchir l'affichage
            await self._update_live_display_unsafe()

    # Raccourcis pour les types de messages courants
    async def show_error(self, message: str): await self.show_message_in_live(ProgressStatus.ERROR, message)
    async def show_warning(self, message: str): await self.show_message_in_live(ProgressStatus.WARNING, message)
    async def show_info(self, message: str): await self.show_message_in_live(ProgressStatus.INFO, message)
    async def show_success(self, message: str): await self.show_message_in_live(ProgressStatus.DONE, message)
    async def show_quota_exceeded(self, message: str): await self.show_message_in_live(ProgressStatus.QUOTA, message)
    async def show_skipped(self, message: str): await self.show_message_in_live(ProgressStatus.SKIPPED, message)

    def show_file_table(self, files: List[Tuple[Path, int, int]]) -> None:
        """Affiche un tableau r√©capitulatif des fichiers de sortie cr√©√©s."""
        # Ne rien afficher si aucun fichier n'a √©t√© cr√©√©
        if not files:
            self.console.print(f"\n{self.ICONS[ProgressStatus.INFO]} [blue]INFO: Aucun fichier de donn√©es cr√©√©.[/]")
            return

        self.console.print("\n[bold underline]Fichiers Cr√©√©s :[/]")
        table = Table(show_header=True, header_style="bold magenta", border_style="dim", expand=True)
        table.add_column("Fichier Relatif", style="cyan", no_wrap=False, overflow="fold", min_width=40)
        table.add_column("Tokens (estim√©s)", style="blue", justify="right")
        table.add_column("Vid√©os", style="green", justify="right")

        total_tokens, total_videos = 0, 0
        for file_path, tokens, video_count in files:
            try:
                # Afficher le chemin relatif au dossier de travail pour la concision
                display_path = str(file_path.relative_to(config.WORK_FOLDER))
            except ValueError:
                display_path = str(file_path) # Chemin absolu si non relatif
            # Formater les nombres pour la lisibilit√©
            table.add_row(display_path, f"{tokens:,}".replace(",", " "), str(video_count))
            total_tokens += tokens
            total_videos += video_count

        # Ajouter une ligne de total
        table.add_row(
            Text("TOTAL", style="bold"),
            Text(f"{total_tokens:,}".replace(",", " "), style="bold blue"),
            Text(str(total_videos), style="bold green")
        )
        self.console.print(table)

    async def _update_live_display_unsafe(self) -> None:
        """M√©thode interne pour mettre √† jour l'affichage Rich Live (doit √™tre appel√©e sous verrou)."""
        if self.live and self.live.is_started:
            try:
                # Cr√©er un groupe Rich avec toutes les lignes actuelles
                renderable = Group(*self.output_lines)
                # Mettre √† jour le contenu du Live display
                self.live.update(renderable, refresh=True)
            except Exception as e:
                logger.error(f"Erreur lors de la mise √† jour de l'affichage live: {e}", exc_info=True)

    async def stop(self) -> None:
        """Arr√™te proprement l'affichage Live."""
        async with self._lock: # Verrouiller pour √©viter les conditions de course
            if self.live and self.live.is_started:
                try:
                    self.live.stop() # Arr√™ter le rafra√Æchissement
                    logger.debug("Affichage Live arr√™t√©.")
                except Exception as e:
                    logger.error(f"Erreur lors de l'arr√™t de l'affichage Live: {e}")
            self.live = None # R√©initialiser l'instance Live

# ==============================================================================
# SECTION 5 : Client API YouTube
# ==============================================================================

class ContentType:
    """Constantes simples pour les types de contenu YouTube."""
    CHANNEL = "channel"; VIDEO = "video"; PLAYLIST = "playlist"; SEARCH = "search"
    # Types internes avant r√©solution vers un ID de cha√Æne
    _CHANNEL_HANDLE = "channel_handle"; _CHANNEL_CUSTOM = "channel_custom"; _CHANNEL_USER = "channel_user"

class YouTubeAPIClient:
    """G√®re les interactions avec l'API YouTube Data v3."""
    # Patterns Regex pour identifier les types d'URL YouTube
    URL_PATTERNS: Dict[str, str] = {
        ContentType._CHANNEL_HANDLE: r"(?:https?://)?(?:www\.)?youtube\.com/@(?P<identifier>[a-zA-Z0-9_.-]+)",
        ContentType.VIDEO: r"(?:https?://)?(?:www\.)?(?:youtube\.com/watch\?v=|youtu\.be/|youtube\.com/shorts/)(?P<identifier>[a-zA-Z0-9_-]{11})",
        ContentType.PLAYLIST: r"(?:https?://)?(?:www\.)?youtube\.com/(?:playlist|watch)\?.*?list=(?P<identifier>[a-zA-Z0-9_-]+)",
        ContentType.CHANNEL: r"(?:https?://)?(?:www\.)?youtube\.com/channel/(?P<identifier>UC[a-zA-Z0-9_-]+)", # ID de cha√Æne direct
        ContentType._CHANNEL_CUSTOM: r"(?:https?://)?(?:www\.)?youtube\.com/c/(?P<identifier>[a-zA-Z0-9_.-]+)",
        ContentType._CHANNEL_USER: r"(?:https?://)?(?:www\.)?youtube\.com/user/(?P<identifier>[a-zA-Z0-9_.-]+)",
        ContentType.SEARCH: r"(?:https?://)?(?:www\.)?youtube\.com/results\?search_query=(?P<query>[^&]+)" # Recherche via param√®tre URL
    }
    # Types n√©cessitant un appel API suppl√©mentaire pour r√©soudre en ID de cha√Æne
    RESOLVABLE_TYPES: Set[str] = {ContentType._CHANNEL_HANDLE, ContentType._CHANNEL_CUSTOM, ContentType._CHANNEL_USER}
    # Mapping des types bruts (internes ou directs) vers les types de contenu finaux
    CONTENT_TYPE_MAP: Dict[str, str] = {
        ContentType._CHANNEL_HANDLE: ContentType.CHANNEL, ContentType.VIDEO: ContentType.VIDEO,
        ContentType.PLAYLIST: ContentType.PLAYLIST, ContentType.CHANNEL: ContentType.CHANNEL,
        ContentType._CHANNEL_CUSTOM: ContentType.CHANNEL, ContentType._CHANNEL_USER: ContentType.CHANNEL,
        ContentType.SEARCH: ContentType.SEARCH
    }

    def __init__(self, api_key: str):
        """
        Initialise le client API YouTube.

        Args:
            api_key: La cl√© API YouTube Data v3.

        Raises:
            ValueError: Si la cl√© API est manquante ou si l'initialisation √©choue.
        """
        logger.info("Initialisation du client API YouTube (Async)")
        if not api_key:
            logger.critical("Cl√© API manquante.")
            raise ValueError("Cl√© API YouTube non fournie.")
        try:
            # D√©sactiver le cache de d√©couverte pour √©viter probl√®mes potentiels avec async/threads
            self.youtube: Resource = build("youtube", "v3", developerKey=api_key, cache_discovery=False)
            logger.debug("Objet Ressource API YouTube cr√©√© avec succ√®s.")
        except Exception as e:
            logger.critical(f"√âchec de l'initialisation de l'API YouTube: {e}", exc_info=True)
            raise ValueError("Impossible d'initialiser l'API YouTube. V√©rifiez la cl√©/connexion.") from e

        # G√©n√©rateur pour les d√©lais al√©atoires entre les appels API
        self._delay_generator: Callable[[], Coroutine[Any, Any, None]] = \
            lambda: asyncio.sleep(random.uniform(config.MIN_DELAY, config.MAX_DELAY) / 1000.0)

        # Cache FIFO simple pour les pages playlistItems afin de r√©duire les appels redondants lors de la pagination
        self._playlist_item_cache: Dict[Tuple[str, Optional[str]], Dict[str, Any]] = {}
        self._playlist_item_cache_lock: asyncio.Lock = asyncio.Lock() # Verrou pour l'acc√®s au cache

    async def _execute_api_call(self, api_request: Any, cost: int = 1) -> Dict[str, Any]:
        """
        Ex√©cute une requ√™te API googleapiclient de mani√®re asynchrone dans un thread s√©par√©,
        g√®re les erreurs HTTP courantes et applique un d√©lai.

        Args:
            api_request: L'objet requ√™te googleapiclient (ex: youtube.videos().list(...)).
            cost: Un co√ªt indicatif pour la journalisation (non utilis√© par l'API).

        Returns:
            Le dictionnaire de r√©ponse de l'API.

        Raises:
            QuotaExceededError: Si le quota API est d√©pass√©.
            ValueError: Pour les erreurs de configuration/permission (403).
            HttpError: Pour les autres erreurs HTTP non g√©r√©es.
            Exception: Pour les erreurs inattendues lors de l'ex√©cution.
        """
        request_uri = getattr(api_request, 'uri', 'URI Inconnu') # Obtenir l'URI pour les logs
        logger.debug(f"Ex√©cution de l'appel API (co√ªt ~{cost}): {request_uri}")
        try:
            # Ex√©cuter la m√©thode synchrone execute() dans un thread
            response: Dict[str, Any] = await asyncio.to_thread(api_request.execute)
            await self._delay_generator() # Appliquer le d√©lai apr√®s un appel r√©ussi
            return response
        except HttpError as e:
            # Analyser l'erreur HTTP
            status_code = e.resp.status
            content_bytes = getattr(e, 'content', b'') # Contenu de la r√©ponse d'erreur
            content_str = content_bytes.decode('utf-8', errors='replace')
            uri = getattr(e, 'uri', 'URI Inconnu') # Redondant mais s√ªr

            if status_code == 403: # Erreur Forbidden
                if 'quotaExceeded' in content_str or 'servingLimitExceeded' in content_str:
                    logger.critical(f"Quota API YouTube d√©pass√© d√©tect√© (URI: {uri}).")
                    raise QuotaExceededError("Quota API YouTube d√©pass√©.") from e
                elif 'forbidden' in content_str or 'accessNotConfigured' in content_str:
                     logger.error(f"Erreur API 403 Forbidden/AccessNotConfigured: {uri} - V√©rifiez permissions/restrictions cl√© API.", exc_info=False)
                     raise ValueError(f"Acc√®s interdit/non configur√© (403) √† {uri}. V√©rifiez la cl√© API.") from e
                else:
                    # Autre erreur 403 non sp√©cifique
                    logger.error(f"Erreur API 403 non g√©r√©e: {uri} - {content_str}", exc_info=False)
                    raise e # Remonter l'erreur 403 g√©n√©rique
            elif status_code == 404: # Erreur Not Found
                logger.warning(f"Ressource API non trouv√©e (404): {uri}")
                raise e # Remonter pour une gestion sp√©cifique par l'appelant
            else:
                # Autres erreurs HTTP (ex: 500, 503)
                logger.error(f"Erreur API {status_code}: {uri} - {content_str}", exc_info=False)
                raise e # Remonter les autres erreurs HTTP
        except Exception as e:
            # Erreurs inattendues (ex: probl√®me r√©seau avant l'appel)
            logger.error(f"Erreur inattendue lors de l'ex√©cution de l'appel API ({request_uri}): {e}", exc_info=True)
            raise

    @functools.lru_cache(maxsize=config.URL_PARSE_CACHE_SIZE)
    def extract_identifier_sync(self, url: str) -> Optional[Tuple[str, str]]:
        """
        Analyse de mani√®re synchrone une URL pour trouver l'identifiant YouTube et son type brut.
        Utilise un cache LRU pour √©viter les analyses r√©p√©t√©es.

        Args:
            url: L'URL YouTube √† analyser.

        Returns:
            Un tuple (identifiant, type_brut) ou None si aucun pattern ne correspond.
        """
        logger.spam(f"Analyse URL (sync pour cache): {url}")
        if not url or not isinstance(url, str): return None
        try:
            # Priorit√© 1: Recherche via /results?search_query=
            parsed_url = urlparse(url)
            if 'youtube.com' in parsed_url.netloc and parsed_url.path == '/results':
                query_params = parse_qs(parsed_url.query)
                query = query_params.get('search_query', [None])[0]
                if query:
                    decoded_query = unquote_plus(query)
                    logger.debug(f"Type Recherche d√©tect√© (param√®tre URL): '{decoded_query}'")
                    return decoded_query, ContentType.SEARCH

            # Priorit√© 2: Patterns sp√©cifiques (vid√©o, playlist, ID de cha√Æne) - L'ordre est important
            ordered_patterns = [ContentType.VIDEO, ContentType.PLAYLIST, ContentType.CHANNEL]
            for type_key in ordered_patterns:
                match = re.match(self.URL_PATTERNS[type_key], url)
                if match:
                    identifier = match.groupdict().get("identifier")
                    if identifier: # V√©rifier que l'identifiant a √©t√© captur√©
                        logger.debug(f"Pattern '{type_key}' trouv√©. Type: {type_key}, ID: {identifier}")
                        return identifier, type_key

            # Priorit√© 3: Patterns n√©cessitant r√©solution (handle, custom, user)
            resolvable_patterns = [ContentType._CHANNEL_HANDLE, ContentType._CHANNEL_CUSTOM, ContentType._CHANNEL_USER]
            for type_key in resolvable_patterns:
                 match = re.match(self.URL_PATTERNS[type_key], url)
                 if match:
                     identifier = match.groupdict().get("identifier")
                     if identifier:
                         logger.debug(f"Pattern '{type_key}' trouv√© (n√©cessite r√©solution). Type brut: {type_key}, ID brut: {identifier}")
                         return identifier, type_key # Retourner le type brut ici

            # Solution de repli: Pattern de recherche g√©n√©rique (si non d√©tect√© via /results?)
            # Ce regex pourrait √™tre moins sp√©cifique, donc en dernier recours
            search_pattern_fallback = r"(?:https?://)?(?:www\.)?youtube\.com/results\?.*?search_query=(?P<query>[^&]+)"
            match_fallback = re.match(search_pattern_fallback, url)
            if match_fallback and match_fallback.groupdict().get("query"):
                 decoded_query = unquote_plus(match_fallback.group("query"))
                 logger.debug(f"Type Recherche d√©tect√© (pattern de repli): '{decoded_query}'")
                 return decoded_query, ContentType.SEARCH

            # Si aucun pattern ne correspond
            logger.warning(f"Aucun pattern YouTube valide trouv√© pour l'URL: {url}")
            return None
        except Exception as e:
            # G√©rer les erreurs potentielles lors de l'analyse regex/urlparse
            logger.error(f"Erreur lors de l'analyse de l'URL '{url}': {e}", exc_info=True)
            return None

    async def extract_identifier(self, url: str) -> Optional[Tuple[str, str]]:
        """
        Extrait de mani√®re asynchrone l'identifiant et le type de contenu final d'une URL YouTube.
        R√©sout les handles, URLs personnalis√©es et URLs utilisateur en ID de cha√Æne.

        Args:
            url: L'URL YouTube √† traiter.

        Returns:
            Un tuple (identifiant, type_contenu_final) ou None si invalide ou √©chec de r√©solution.
            Le type_contenu_final sera l'une des valeurs de ContentType (CHANNEL, VIDEO, PLAYLIST, SEARCH).
        """
        # Utiliser la version synchrone mise en cache pour l'analyse initiale
        sync_result = self.extract_identifier_sync(url)
        if not sync_result: return None # URL invalide ou non reconnue

        identifier, raw_type_key = sync_result

        # Si le type n√©cessite une r√©solution (handle, custom, user)
        if raw_type_key in self.RESOLVABLE_TYPES:
            logger.info(f"R√©solution n√©cessaire pour {raw_type_key}: {identifier}")
            try:
                # Appeler la m√©thode de r√©solution asynchrone
                channel_id = await self._resolve_channel_identifier(raw_type_key, identifier)
                if channel_id:
                    logger.info(f"'{identifier}' ({raw_type_key}) r√©solu en ID de Cha√Æne: {channel_id}")
                    # Le type final est 'channel' apr√®s r√©solution
                    return channel_id, ContentType.CHANNEL
                else:
                    # La r√©solution a √©chou√© (ex: handle/custom URL invalide)
                    logger.warning(f"√âchec de la r√©solution de '{identifier}' ({raw_type_key}) en ID de Cha√Æne.")
                    return None
            except QuotaExceededError:
                raise # Propager imm√©diatement les erreurs de quota
            except Exception as e:
                # G√©rer les erreurs inattendues pendant la r√©solution
                logger.error(f"Erreur lors de la r√©solution asynchrone de {identifier} ({raw_type_key}): {e}", exc_info=True)
                return None
        else:
             # Pour les types non r√©solvables (video, playlist, channel_id, search)
             # Mapper le type brut vers le type de contenu final (ex: channel_id -> channel)
             final_content_type = self.CONTENT_TYPE_MAP.get(raw_type_key, raw_type_key)
             return identifier, final_content_type

    @functools.lru_cache(maxsize=config.RESOLVE_CACHE_SIZE)
    def _resolve_channel_identifier_sync_logic(self, identifier_type: str, identifier: str) -> Optional[str]:
        """
        Logique synchrone pour r√©soudre les handles/URLs personnalis√©es/utilisateurs en ID de cha√Æne.
        Utilise un cache LRU. C'est cette fonction qui est appel√©e via asyncio.to_thread.

        Args:
            identifier_type: Le type brut (_CHANNEL_HANDLE, _CHANNEL_CUSTOM, _CHANNEL_USER).
            identifier: L'identifiant extrait de l'URL (handle, nom custom, nom utilisateur).

        Returns:
            L'ID de cha√Æne (UC...) ou None si non trouv√© ou erreur.

        Raises:
            HttpError: Si une erreur API se produit (sera attrap√©e par l'appelant async).
        """
        log_prefix = f"R√©solution {identifier_type} '{identifier}' (logique sync)"
        try:
            # Les Handles (@nom) et URLs Custom (/c/nom) sont r√©solus via l'API Search
            # C'est la m√©thode recommand√©e actuellement par Google.
            if identifier_type in [ContentType._CHANNEL_HANDLE, ContentType._CHANNEL_CUSTOM]:
                 logger.debug(f"{log_prefix}: Tentative de r√©solution via l'API Search...")
                 # Utiliser 'fields' pour minimiser la r√©ponse et le co√ªt quota
                 resp = self.youtube.search().list(
                     part="id", # Seul l'ID est n√©cessaire
                     q=identifier, # Rechercher l'identifiant
                     type="channel", # Chercher sp√©cifiquement une cha√Æne
                     maxResults=1, # On ne veut que le r√©sultat le plus pertinent
                     fields="items(id/channelId)" # Ne r√©cup√©rer que l'ID de la cha√Æne
                 ).execute()
                 items = resp.get("items", [])
                 if items:
                     channel_id = items[0].get("id", {}).get("channelId")
                     logger.info(f"{log_prefix}: ID de Cha√Æne {channel_id} trouv√© via l'API Search.")
                     return channel_id
                 else:
                     # L'API Search n'a pas trouv√© de correspondance exacte
                     logger.warning(f"{log_prefix}: Non trouv√© via l'API Search.")
                     return None

            # Les URLs User (/user/nom) sont r√©solues via channels().list(forUsername=...)
            elif identifier_type == ContentType._CHANNEL_USER:
                logger.debug(f"{log_prefix}: Tentative de r√©solution via l'API Channels (forUsername)...")
                resp = self.youtube.channels().list(
                    part="id", # Seul l'ID est n√©cessaire
                    forUsername=identifier,
                    fields="items(id)" # Ne r√©cup√©rer que l'ID
                ).execute()
                items = resp.get("items", [])
                if items:
                    channel_id = items[0].get("id")
                    logger.info(f"{log_prefix}: ID de Cha√Æne {channel_id} trouv√© via l'API Channels (forUsername).")
                    return channel_id
                else:
                    # L'API Channels n'a pas trouv√© de correspondance pour cet username
                    logger.warning(f"{log_prefix}: Non trouv√© via l'API Channels (forUsername).")
                    return None
            else:
                # Cas non pr√©vu
                logger.error(f"Type d'identifiant non support√© pour la r√©solution: {identifier_type}")
                return None

        except HttpError as e:
            # Erreur API pendant la r√©solution synchrone
            uri = getattr(e, 'uri', 'URI Inconnu')
            logger.warning(f"{log_prefix}: Erreur API (sync) {e.resp.status}: {uri}")
            raise e # Remonter l'erreur pour √™tre g√©r√©e par l'appelant async
        except Exception as e:
            # Erreur inattendue dans la logique synchrone
            logger.error(f"{log_prefix}: Erreur synchrone inattendue: {e}", exc_info=True)
            raise # Remonter

    async def _resolve_channel_identifier(self, identifier_type: str, identifier: str) -> Optional[str]:
        """
        R√©sout de mani√®re asynchrone un handle, une URL personnalis√©e ou une URL utilisateur en ID de cha√Æne.
        G√®re les erreurs API sp√©cifiques √† la r√©solution.

        Args:
            identifier_type: Le type brut (_CHANNEL_HANDLE, _CHANNEL_CUSTOM, _CHANNEL_USER).
            identifier: L'identifiant √† r√©soudre.

        Returns:
            L'ID de cha√Æne (UC...) ou None si √©chec.

        Raises:
            QuotaExceededError: Si le quota est d√©pass√© pendant la r√©solution.
            ValueError: Si une erreur de permission 403 se produit.
        """
        log_prefix = f"R√©solution async {identifier_type} '{identifier}'"
        try:
            # Ex√©cuter la logique synchrone (mise en cache) dans un thread s√©par√©
            channel_id: Optional[str] = await asyncio.to_thread(
                self._resolve_channel_identifier_sync_logic,
                identifier_type,
                identifier
            )
            return channel_id
        except HttpError as e:
            # G√©rer les erreurs HTTP sp√©cifiques √† la r√©solution
            if e.resp.status == 404:
                logger.warning(f"{log_prefix}: Identifiant non trouv√© (404).")
                return None # L'identifiant n'existe pas
            elif e.resp.status == 403:
                 # V√©rifier si c'est une erreur de quota ou une autre erreur 403
                 content_bytes = getattr(e, 'content', b'')
                 if b'quotaExceeded' in content_bytes or b'servingLimitExceeded' in content_bytes:
                     raise QuotaExceededError from e # Propager l'erreur de quota
                 else:
                     # Autre erreur 403 (permissions, cl√© invalide, etc.)
                     raise ValueError(f"Acc√®s interdit (403) lors de la r√©solution de {identifier_type} '{identifier}'. V√©rifiez la cl√© API.") from e
            else:
                # Autres erreurs HTTP non g√©r√©es sp√©cifiquement
                logger.error(f"{log_prefix}: Erreur API non g√©r√©e {e.resp.status}: {e}")
                raise # Remonter l'erreur
        except QuotaExceededError:
            raise # Propager imm√©diatement si lev√©e par la logique sync
        except Exception as e:
            # G√©rer les erreurs inattendues pendant l'ex√©cution asynchrone
            logger.error(f"{log_prefix}: Erreur asynchrone inattendue: {e}", exc_info=True)
            return None # Consid√©rer comme un √©chec de r√©solution

    async def get_videos_from_source(self, source_type: str, source_id_or_query: str) -> Tuple[List[str], str]:
        """
        R√©cup√®re une liste d'IDs de vid√©os potentiels bas√©s sur le type de source
        (cha√Æne, playlist, vid√©o unique, recherche).
        Effectue un filtrage initial (ex: par date pour les playlists).

        Args:
            source_type: Le type de contenu (ContentType.CHANNEL, etc.).
            source_id_or_query: L'ID de la ressource ou la requ√™te de recherche.

        Returns:
            Un tuple contenant :
            - Une liste d'IDs de vid√©os.
            - Un nom descriptif pour la source (ex: titre playlist, nom cha√Æne).

        Raises:
            QuotaExceededError: Si le quota est d√©pass√©.
            ValueError: Si le type de source est invalide ou si une ressource n'est pas trouv√©e (ex: playlist 404).
            HttpError: Pour d'autres erreurs API non g√©r√©es.
        """
        logger.info(f"R√©cup√©ration asynchrone des IDs vid√©o pour {source_type}: {source_id_or_query[:100]}...")
        video_ids: List[str] = []
        source_name = f"{source_type.capitalize()}: {source_id_or_query[:50]}" # Nom par d√©faut

        try:
            if source_type == ContentType.CHANNEL:
                channel_id = source_id_or_query
                # R√©cup√©rer l'ID de la playlist "Uploads" et le nom de la cha√Æne
                uploads_playlist_id, source_name = await self._get_channel_uploads_playlist_id(channel_id)
                if not uploads_playlist_id:
                    raise ValueError(f"Impossible de trouver l'ID de la playlist 'uploads' pour l'ID de cha√Æne {channel_id}")
                logger.info(f"R√©cup√©ration des vid√©os de la playlist uploads '{uploads_playlist_id}' (Cha√Æne: '{source_name}')")
                # Utiliser l'it√©rateur asynchrone pour r√©cup√©rer les IDs
                video_ids = [vid_id async for vid_id in self._yield_video_ids_from_playlist(uploads_playlist_id)]

            elif source_type == ContentType.PLAYLIST:
                playlist_id = source_id_or_query
                try:
                    # Essayer de r√©cup√©rer le titre de la playlist pour un meilleur nom de source
                    req = self.youtube.playlists().list(part="snippet", id=playlist_id, fields="items(snippet/title)")
                    resp = await self._execute_api_call(req, cost=1)
                    # Mettre √† jour source_name si le titre est trouv√©
                    if resp.get('items'):
                        source_name = resp['items'][0].get('snippet', {}).get('title', source_name)
                except HttpError as e:
                    # G√©rer sp√©cifiquement le cas o√π la playlist n'existe pas
                    if e.resp.status == 404:
                        raise ValueError(f"Playlist non trouv√©e (ID: {playlist_id})") from e
                    else:
                        raise # Propager les autres erreurs API
                logger.info(f"R√©cup√©ration des vid√©os de la playlist '{source_name}' (ID: {playlist_id})")
                video_ids = [vid_id async for vid_id in self._yield_video_ids_from_playlist(playlist_id)]

            elif source_type == ContentType.VIDEO:
                video_id = source_id_or_query
                # V√©rifier si la vid√©o unique respecte les crit√®res de base (date, dur√©e, pas en live)
                is_valid, source_name = await self._check_single_video_validity(video_id)
                if is_valid:
                    video_ids = [video_id] # Liste contenant seulement cet ID
                else:
                    # La vid√©o unique ne correspond pas, on retourne une liste vide
                    logger.info(f"La vid√©o unique {video_id} ('{source_name}') ne correspond pas aux crit√®res, ignor√©e.")
                    video_ids = []

            elif source_type == ContentType.SEARCH:
                # R√©cup√©rer les IDs via la fonction de recherche
                video_ids, source_name = await self._search_videos(source_id_or_query)

            else:
                # G√©rer les types de source inconnus
                raise ValueError(f"Type de source non support√© : {source_type}")

            logger.info(f"{len(video_ids)} ID(s) vid√©o potentiel(s) trouv√©(s) pour la source '{source_name}'.")
            return video_ids, source_name

        except QuotaExceededError:
            raise # Propager imm√©diatement
        except (ValueError, HttpError) as e:
            # Erreurs attendues (ex: playlist non trouv√©e, type invalide)
            logger.error(f"Erreur lors de la r√©cup√©ration des vid√©os pour la source {source_type} '{source_id_or_query}': {e}")
            raise # Remonter pour gestion dans le moteur principal
        except Exception as e:
            # Erreurs inattendues
            logger.error(f"Erreur inattendue lors de la r√©cup√©ration des vid√©os: {e}", exc_info=True)
            raise # Remonter

    async def _get_channel_uploads_playlist_id(self, channel_id: str) -> Tuple[Optional[str], str]:
        """R√©cup√®re l'ID de la playlist 'uploads' et le titre de la cha√Æne pour un ID de cha√Æne donn√©."""
        default_name = f"Cha√Æne ID: {channel_id}" # Nom par d√©faut si le titre n'est pas trouv√©
        try:
            req = self.youtube.channels().list(
                part="snippet,contentDetails", # Besoin du snippet pour le titre, contentDetails pour la playlist uploads
                id=channel_id,
                fields="items(snippet/title,contentDetails/relatedPlaylists/uploads)" # Minimiser les champs
            )
            resp = await self._execute_api_call(req, cost=1) # Co√ªt faible
            if not resp.get('items'):
                logger.warning(f"Cha√Æne non trouv√©e (ID: {channel_id}) lors de la r√©cup√©ration de la playlist uploads.")
                return None, default_name
            # Extraire les informations de la r√©ponse
            item = resp['items'][0]
            channel_name = item.get('snippet', {}).get('title', default_name)
            playlist_id = item.get('contentDetails', {}).get('relatedPlaylists', {}).get('uploads')
            if not playlist_id:
                 # Certaines cha√Ænes peuvent ne pas avoir de playlist uploads visible
                 logger.warning(f"Impossible de trouver l'ID de la playlist uploads pour la cha√Æne '{channel_name}' ({channel_id}).")
            return playlist_id, channel_name
        except HttpError as e:
            # G√©rer sp√©cifiquement l'erreur 404 (cha√Æne non trouv√©e)
            if e.resp.status == 404:
                logger.warning(f"Cha√Æne non trouv√©e (404) pour ID: {channel_id}")
                return None, default_name
            else:
                raise # Propager les autres erreurs API
        except Exception as e:
            # G√©rer les erreurs inattendues
            logger.error(f"Erreur lors de la r√©cup√©ration de l'ID de la playlist uploads pour {channel_id}: {e}", exc_info=True)
            return None, default_name # Retourner None en cas d'erreur

    async def _check_single_video_validity(self, video_id: str) -> Tuple[bool, str]:
        """V√©rifie si une vid√©o unique respecte les crit√®res de date, dur√©e et statut live."""
        default_name = f"Vid√©o ID: {video_id}" # Nom par d√©faut
        try:
            # Demander les champs n√©cessaires pour la validation
            fields = "items(id,snippet(title,publishedAt,liveBroadcastContent),contentDetails/duration)"
            req = self.youtube.videos().list(part="snippet,contentDetails", id=video_id, fields=fields)
            resp = await self._execute_api_call(req, cost=1) # Co√ªt faible pour une seule vid√©o

            if not resp.get('items'):
                logger.warning(f"Vid√©o non trouv√©e (ID: {video_id}) lors de la v√©rification de validit√©.")
                return False, default_name
            # Extraire les donn√©es de la r√©ponse
            item = resp['items'][0]
            video_name = item.get('snippet', {}).get('title', default_name)
            duration_iso = item.get('contentDetails', {}).get('duration')
            live_status = item.get('snippet', {}).get('liveBroadcastContent', 'none') # 'none', 'live', 'upcoming', 'completed'
            published_at_str = item.get('snippet', {}).get('publishedAt')

            # V√©rifier la date de publication
            date_ok = False
            if published_at_str:
                try:
                    ts = published_at_str
                    if not ts.endswith('Z'): ts += 'Z' # Assurer format UTC
                    pub_dt = datetime.strptime(ts, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc)
                    date_ok = pub_dt >= config.LIMIT_DATE # Comparer √† la date limite configur√©e
                except (ValueError, TypeError):
                    logger.warning(f"Format de date invalide '{published_at_str}' pour la vid√©o {video_id}.")
                    date_ok = False # Consid√©rer comme invalide si la date ne peut √™tre pars√©e

            # V√©rifier la dur√©e et le statut live en utilisant l'helper
            duration_live_ok = self._is_valid_video(duration_iso, live_status)

            # La vid√©o est valide si la date ET la dur√©e/statut sont OK
            return date_ok and duration_live_ok, video_name

        except HttpError as e:
            # G√©rer l'erreur 404 (vid√©o non trouv√©e)
            if e.resp.status == 404:
                logger.warning(f"Vid√©o non trouv√©e (404) pour ID: {video_id}")
                return False, default_name
            else:
                raise # Propager les autres erreurs API
        except Exception as e:
            # G√©rer les erreurs inattendues
            logger.error(f"Erreur lors de la v√©rification de validit√© de la vid√©o {video_id}: {e}", exc_info=True)
            return False, default_name # Consid√©rer comme invalide en cas d'erreur

    async def _search_videos(self, query: str, max_results: Optional[int] = None) -> Tuple[List[str], str]:
        """Effectue une recherche YouTube et r√©cup√®re les IDs des vid√©os, g√©rant la pagination et les filtres."""
        logger.info(f"Ex√©cution de la recherche asynchrone pour la requ√™te : '{query}'")
        max_r = max_results if max_results is not None else config.MAX_SEARCH_RESULTS
        # Si max_results est 0 ou n√©gatif, retourner imm√©diatement
        if max_r <= 0: return [], f"Recherche: {query} (0 r√©sultats demand√©s)"

        # Analyser la requ√™te pour s√©parer les mots-cl√©s des filtres API
        parsed_query, api_params = self._parse_search_query(query)
        video_ids: List[str] = []
        next_page_token: Optional[str] = None
        retrieved_count = 0
        # Construire un nom de source descriptif incluant les filtres
        filter_desc = f" ({len(api_params)} filtre(s))" if api_params else ""
        source_name = f"Recherche '{parsed_query}'{filter_desc}"

        # Boucler tant qu'il y a des pages et que la limite n'est pas atteinte
        while retrieved_count < max_r:
            try:
                # Calculer combien de r√©sultats demander pour ce lot
                batch_size = min(config.BATCH_SIZE, max_r - retrieved_count)
                # Si batch_size est 0 ou moins, on a atteint la limite
                if batch_size <= 0: break

                # Pr√©parer les param√®tres pour l'appel search.list
                params = {
                    "q": parsed_query, # Mots-cl√©s principaux
                    "part": "id", # On ne veut que l'ID de la vid√©o
                    "type": "video", # Chercher uniquement des vid√©os
                    "maxResults": batch_size,
                    "pageToken": next_page_token, # Pour la pagination
                    **api_params # Ajouter les filtres pars√©s (ex: publishedAfter)
                }
                # Minimiser les champs retourn√©s pour √©conomiser quota/bande passante
                params["fields"] = "items(id/videoId),nextPageToken"
                logger.debug(f"Pr√©paration de l'appel API Search avec params: {params}")

                req = self.youtube.search().list(**params)
                # L'API Search a un co√ªt quota √©lev√© (100 unit√©s)
                resp = await self._execute_api_call(req, cost=100)

                # Extraire les IDs de vid√©os de la r√©ponse
                items = resp.get('items', [])
                new_ids = [item['id']['videoId'] for item in items if item.get('id', {}).get('videoId')]
                video_ids.extend(new_ids)
                retrieved_count += len(new_ids)
                logger.debug(f"Lot de recherche a retourn√© {len(new_ids)} IDs. Total r√©cup√©r√©: {retrieved_count}/{max_r}.")

                # R√©cup√©rer le token pour la page suivante
                next_page_token = resp.get('nextPageToken')
                if not next_page_token:
                    logger.debug("Plus de pages dans les r√©sultats de recherche.")
                    break # Sortir de la boucle while
            except HttpError as e:
                 # G√©rer sp√©cifiquement l'erreur "invalid search filter"
                 content_bytes = getattr(e, 'content', b'')
                 if e.resp.status == 400 and b'invalid search filter' in content_bytes:
                     logger.error(f"Erreur de recherche (400 Bad Request - Filtre invalide?): Requ√™te='{query}', Params={api_params}")
                     # Lever une erreur plus descriptive
                     raise ValueError(f"Filtre de recherche invalide fourni pour la requ√™te '{query}'.") from e
                 else:
                     raise # Propager les autres erreurs API
            except QuotaExceededError:
                raise # Propager imm√©diatement
            except Exception as e:
                # G√©rer les erreurs inattendues pendant la recherche
                logger.error(f"Erreur inattendue lors de la recherche pour '{query}': {e}", exc_info=True)
                break # Arr√™ter la recherche en cas d'erreur inattendue

        logger.info(f"Recherche pour '{query}' termin√©e. Trouv√© {len(video_ids)} IDs vid√©o.")
        return video_ids, source_name

    def _parse_search_query(self, query: str) -> Tuple[str, Dict[str, Any]]:
        """
        Analyse une cha√Æne de requ√™te de recherche pour s√©parer les mots-cl√©s
        des op√©rateurs de filtre de l'API (ex: before:, after:, channel:, etc.).

        Args:
            query: La cha√Æne de requ√™te brute entr√©e par l'utilisateur.

        Returns:
            Un tuple contenant :
            - La cha√Æne de requ√™te nettoy√©e ('q' pour l'API).
            - Un dictionnaire des param√®tres API d√©riv√©s des op√©rateurs.
        """
        logger.debug(f"Analyse de la requ√™te de recherche : {query}")
        api_params: Dict[str, Any] = {}
        # Mapping des pr√©fixes op√©rateurs vers les noms de param√®tres API et validateurs/formateurs optionnels
        op_map: Dict[str, Optional[Tuple[str, Optional[Callable[[str], Optional[Any]]]]]] = {
            'intitle:': None, 'description:': None, # Ceux-ci sont ajout√©s au param√®tre 'q'
            'before:': ('publishedBefore', self._format_date_for_api), # Date avant YYYY-MM-DD
            'after:': ('publishedAfter', self._format_date_for_api),   # Date apr√®s YYYY-MM-DD
            'channel:': ('channelId', None), # Attend un ID de cha√Æne (UC...)
            'duration:': ('videoDuration', lambda x: x.lower() if x.lower() in ['short', 'medium', 'long', 'any'] else None),
            'definition:': ('videoDefinition', lambda x: x.lower() if x.lower() in ['high', 'standard', 'any'] else None),
            'license:': ('videoLicense', lambda x: {'creativecommon':'creativeCommon'}.get(x.lower(), x.lower() if x.lower() in ['youtube', 'any'] else 'any')),
            'dimension:': ('videoDimension', lambda x: x.lower() if x.lower() in ['2d', '3d', 'any'] else None),
            'caption:': ('videoCaption', lambda x: {'caption':'closedCaption','nocaption':'none'}.get(x.lower(), 'any')), # 'closedCaption' ou 'none'
            'embeddable:': ('videoEmbeddable', lambda x: {'true':'true','false':'false'}.get(x.lower(), 'any')), # 'true' ou 'false'
            'syndicated:': ('videoSyndicated', lambda x: {'true':'true','false':'false'}.get(x.lower(), 'any')), # 'true' ou 'false'
            'order:': ('order', lambda x: x.lower() if x.lower() in ['date', 'rating', 'relevance', 'title', 'videoCount', 'viewCount'] else None) # Ordre de tri
        }

        # Dictionnaire pour stocker les valeurs trouv√©es pour chaque op√©rateur
        found_operators: Dict[str, List[str]] = {op: [] for op in op_map}
        # Liste pour stocker les parties de la requ√™te qui ne sont pas des op√©rateurs/valeurs
        remaining_query_parts: List[str] = []
        # Regex pour capturer : "phrases entre guillemets", op:"valeur entre guillemets", op:valeursimple, mot_simple
        pattern = re.compile(r'"([^"]+)"|(\w+):"([^"]+)"|(\w+):(\S+)|(\S+)')

        # It√©rer sur toutes les correspondances dans la requ√™te
        for match in pattern.finditer(query):
            groups = match.groups()
            if groups[0] is not None: # "phrase entre guillemets"
                remaining_query_parts.append(f'"{groups[0]}"')
            elif groups[1] is not None: # op:"valeur entre guillemets" (groups[1]=op, groups[2]=val)
                op_key = groups[1].lower() + ':' # Construire la cl√© op√©rateur (ex: 'before:')
                if op_key in op_map: found_operators[op_key].append(groups[2]) # Stocker la valeur si l'op√©rateur est connu
                else: remaining_query_parts.append(match.group(0)) # Sinon, traiter comme un terme normal
            elif groups[3] is not None: # op:valeursimple (groups[3]=op, groups[4]=val)
                op_key = groups[3].lower() + ':'
                if op_key in op_map: found_operators[op_key].append(groups[4])
                else: remaining_query_parts.append(match.group(0))
            elif groups[5] is not None: # mot_simple
                 # V√©rifier si le mot ressemble √† un op√©rateur sans valeur (ex: "before" seul)
                 if (groups[5].lower() + ':') in op_map:
                     logger.warning(f"Op√©rateur '{groups[5]}:' trouv√© sans valeur associ√©e. Ignor√©.")
                 else:
                     # C'est un terme de recherche normal
                     remaining_query_parts.append(groups[5])

        # Traiter les op√©rateurs trouv√©s
        q_final_terms = remaining_query_parts[:] # Commencer avec les termes non-op√©rateurs
        for op, values in found_operators.items():
            if not values: continue # Ignorer si aucune valeur n'a √©t√© trouv√©e pour cet op√©rateur
            value = values[-1] # Utiliser la derni√®re valeur sp√©cifi√©e pour un op√©rateur donn√©

            if op in ['intitle:', 'description:']:
                 # Ajouter ces op√©rateurs directement √† la cha√Æne de requ√™te 'q'
                 q_final_terms.append(f'{op}{value}')
                 logger.debug(f"Ajout du terme '{op}{value}' √† la requ√™te 'q'")
            elif op in op_map and op_map[op]:
                 # Cet op√©rateur correspond √† un param√®tre API
                 api_param_key, formatter = op_map[op]
                 # Formater/valider la valeur si n√©cessaire
                 formatted_value = formatter(value) if formatter else value
                 if formatted_value is not None:
                     # Ajouter au dictionnaire des param√®tres API
                     api_params[api_param_key] = formatted_value
                     logger.debug(f"D√©finition du param√®tre API : {api_param_key} = {formatted_value}")
                 else:
                     # La valeur n'est pas valide pour cet op√©rateur
                     logger.warning(f"Valeur '{value}' invalide pour l'op√©rateur '{op}'. Param√®tre API '{api_param_key}' ignor√©.")

        # Construire la cha√Æne de requ√™te 'q' finale
        final_q = " ".join(q_final_terms).strip()
        logger.debug(f"Requ√™te 'q' finale pour l'API : '{final_q}'")
        logger.debug(f"Param√®tres API finaux : {api_params}")
        return final_q, api_params

    @functools.lru_cache(maxsize=128)
    def _format_date_for_api(self, date_str: str) -> Optional[str]:
        """Formate une cha√Æne YYYY-MM-DD ou YYYYMMDD au format RFC 3339 pour l'API."""
        try:
            # Accepter les deux formats courants
            dt = datetime.strptime(date_str.replace('-', ''), "%Y%m%d")
            # Retourner au format UTC au d√©but de la journ√©e
            return dt.replace(tzinfo=timezone.utc).strftime("%Y-%m-%dT00:00:00Z")
        except (ValueError, TypeError):
            logger.warning(f"Format de date invalide pour le filtre API : '{date_str}'. Attendu YYYY-MM-DD.")
            return None # Retourner None si le format est invalide

    async def _fetch_playlist_page(self, playlist_id: str, page_token: Optional[str]) -> Dict[str, Any]:
        """R√©cup√®re une seule page d'√©l√©ments de playlist, en utilisant un cache manuel."""
        cache_key = (playlist_id, page_token)
        # V√©rifier le cache avant l'appel API
        async with self._playlist_item_cache_lock:
            if cache_key in self._playlist_item_cache:
                logger.spam(f"Cache HIT pour playlist {playlist_id} page {page_token}")
                return self._playlist_item_cache[cache_key]

        logger.debug(f"Cache MISS pour playlist {playlist_id} page {page_token}. Appel API...")
        req = self.youtube.playlistItems().list(
            part="snippet", # snippet contient publishedAt et resourceId.videoId
            playlistId=playlist_id,
            maxResults=config.BATCH_SIZE,
            pageToken=page_token,
            fields="items(snippet(publishedAt,resourceId/videoId)),nextPageToken" # Champs minimaux
        )
        try:
            resp = await self._execute_api_call(req, cost=1) # Co√ªt faible
            # Mettre en cache le r√©sultat apr√®s l'appel r√©ussi
            async with self._playlist_item_cache_lock:
                 # √âviction FIFO simple si le cache est plein
                 if len(self._playlist_item_cache) >= config.PLAYLIST_ITEM_CACHE_SIZE:
                     try:
                         # Retirer l'√©l√©ment le plus ancien (premier ins√©r√©)
                         self._playlist_item_cache.pop(next(iter(self._playlist_item_cache)))
                     except StopIteration: pass # Le cache √©tait vide
                 self._playlist_item_cache[cache_key] = resp
                 logger.spam(f"Page mise en cache pour playlist {playlist_id} page {page_token}")
            return resp
        except HttpError as e:
            # G√©rer l'erreur 404 (playlist non trouv√©e)
            if e.resp.status == 404:
                 logger.warning(f"Playlist {playlist_id} non trouv√©e (404) lors de la r√©cup√©ration de la page {page_token}.")
                 # Lever une erreur sp√©cifique pour √™tre g√©r√©e par l'appelant
                 raise ValueError(f"Playlist {playlist_id} non trouv√©e") from e
            else:
                raise # Propager les autres erreurs API
        except QuotaExceededError:
            raise # Propager imm√©diatement

    async def _yield_video_ids_from_playlist(self, playlist_id: str) -> AsyncIterator[str]:
        """
        It√©rateur asynchrone qui produit les IDs de vid√©os d'une playlist,
        en filtrant par date de publication.
        """
        logger.info(f"R√©cup√©ration asynchrone des IDs vid√©o de la playlist : {playlist_id}")
        next_page_token: Optional[str] = None
        processed_item_count, yielded_count = 0, 0
        limit_dt = config.LIMIT_DATE # Date limite configur√©e
        has_more_pages = True

        # Boucler tant qu'il y a des pages √† r√©cup√©rer
        while has_more_pages:
            try:
                page_token_log = '(premi√®re page)' if next_page_token is None else f"{next_page_token[:10]}..."
                logger.debug(f"R√©cup√©ration du lot playlist {playlist_id}, page token: {page_token_log}")
                # R√©cup√©rer la page (potentiellement depuis le cache)
                resp = await self._fetch_playlist_page(playlist_id, next_page_token)
                items = resp.get('items', [])
                if not items:
                    logger.debug(f"Aucun √©l√©ment retourn√© sur cette page pour la playlist {playlist_id}.")
                    break # Fin de la playlist

                ids_in_page: List[str] = []
                # Drapeau pour arr√™ter la pagination t√¥t si on ne trouve que des vid√©os trop anciennes
                # (Suppose un tri chronologique inverse dans la r√©ponse API, courant pour les uploads)
                stop_pagination_early = False

                # Traiter chaque √©l√©ment de la page
                for item in items:
                    processed_item_count += 1
                    snippet = item.get('snippet', {})
                    video_id = snippet.get('resourceId', {}).get('videoId')
                    published_at_str = snippet.get('publishedAt')

                    # V√©rifier si les donn√©es essentielles sont pr√©sentes
                    if not video_id or not published_at_str:
                        logger.warning(f"√âl√©ment incomplet dans la playlist {playlist_id}: {item}")
                        continue # Ignorer cet √©l√©ment

                    try:
                        # Analyser la date de publication
                        ts = published_at_str
                        if not ts.endswith('Z'): ts += 'Z' # Assurer UTC
                        published_at_dt = datetime.strptime(ts, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.utc)

                        # V√©rifier si la vid√©o est assez r√©cente
                        if published_at_dt >= limit_dt:
                            ids_in_page.append(video_id)
                        else:
                            # Vid√©o trop ancienne
                            logger.log(SPAM_LEVEL_NUM, f"Vid√©o {video_id} de la playlist {playlist_id} est trop ancienne ({published_at_dt}).")
                            # Si cette page ne contient QUE des vid√©os trop anciennes, on peut probablement arr√™ter
                            if not ids_in_page:
                                stop_pagination_early = True

                    except (ValueError, TypeError):
                        # G√©rer les dates invalides
                        logger.warning(f"Format de date invalide '{published_at_str}' pour vid√©o {video_id} dans playlist {playlist_id}.")
                        # Traiter les vid√©os avec date invalide comme ne respectant pas les crit√®res

                # Produire (yield) les IDs valides trouv√©s sur cette page
                for vid_id in ids_in_page:
                    yield vid_id
                    yielded_count += 1

                # Passer √† la page suivante
                next_page_token = resp.get('nextPageToken')
                # Arr√™ter s'il n'y a plus de page ou si on a d√©cid√© d'arr√™ter t√¥t
                if not next_page_token or stop_pagination_early:
                    log_msg = "Pagination de la playlist termin√©e"
                    if stop_pagination_early and next_page_token:
                        log_msg += " (arr√™t anticip√© car vid√©os trop anciennes)"
                    logger.info(f"{log_msg} pour {playlist_id}.")
                    has_more_pages = False

            except QuotaExceededError:
                raise # Propager imm√©diatement
            except ValueError as e: # Erreur sp√©cifique (ex: playlist non trouv√©e)
                 logger.error(f"Erreur lors de la r√©cup√©ration de la playlist {playlist_id}: {e}")
                 raise # Arr√™ter le traitement pour cette source
            except Exception as e:
                # G√©rer les erreurs inattendues pendant la pagination
                page_token_log = '(premi√®re page)' if next_page_token is None else f"{next_page_token[:10]}..."
                logger.error(f"Erreur inattendue playlist {playlist_id} page {page_token_log}: {e}", exc_info=True)
                raise # Arr√™ter le traitement pour cette source

        logger.info(f"Playlist {playlist_id}: {processed_item_count} √©l√©ments trait√©s, {yielded_count} IDs potentiels produits apr√®s filtre date.")

    def _is_valid_video(self, duration_iso: Optional[str], live_status: Optional[str]) -> bool:
        """V√©rifie si une vid√©o respecte les crit√®res de dur√©e et de statut live."""
        # Accepter 'none' (VOD), 'completed' (live termin√©). Exclure 'live', 'upcoming'.
        # Accepter aussi None comme statut (probablement VOD).
        if live_status not in ('none', 'completed', None):
            logger.log(SPAM_LEVEL_NUM, f"Vid√©o filtr√©e (statut live non 'none' ou 'completed'): {live_status}")
            return False

        # V√©rifier la dur√©e
        if not duration_iso:
            logger.log(SPAM_LEVEL_NUM, "Vid√©o filtr√©e (dur√©e manquante)")
            return False
        try:
            duration: timedelta = isodate.parse_duration(duration_iso)
            # V√©rifier si la dur√©e est sup√©rieure ou √©gale √† la dur√©e minimale configur√©e
            is_long_enough = duration >= config.MIN_DURATION
            if not is_long_enough:
                 logger.log(SPAM_LEVEL_NUM, f"Vid√©o filtr√©e (dur√©e {duration} < {config.MIN_DURATION})")
            return is_long_enough
        except (isodate.ISO8601Error, TypeError):
             # G√©rer les formats de dur√©e invalides
             logger.warning(f"Format de dur√©e invalide '{duration_iso}', vid√©o filtr√©e.")
             return False

    async def get_video_details_batch(self, video_ids: List[str]) -> List[Video]:
        """
        R√©cup√®re les informations d√©taill√©es pour une liste d'IDs de vid√©os par lots.
        Filtre les vid√©os bas√©es sur la dur√©e et le statut live apr√®s r√©cup√©ration.

        Args:
            video_ids: Une liste d'IDs de vid√©os √† traiter.

        Returns:
            Une liste d'objets Video valides avec leurs d√©tails.
        """
        if not video_ids: return []
        # Assurer l'unicit√© tout en pr√©servant l'ordre si possible (important si l'ordre source compte)
        unique_ids = list(dict.fromkeys(video_ids))
        logger.info(f"R√©cup√©ration/filtrage asynchrone des d√©tails pour {len(unique_ids)} ID(s) vid√©o unique(s)...")

        valid_videos_with_details: List[Video] = []
        tasks: List[asyncio.Task] = [] # Liste pour stocker les t√¢ches de r√©cup√©ration de lots

        # Cr√©er les t√¢ches pour r√©cup√©rer les lots en parall√®le
        for i in range(0, len(unique_ids), config.BATCH_SIZE):
            batch_ids = unique_ids[i : i + config.BATCH_SIZE]
            # Cr√©er une t√¢che pour chaque lot
            tasks.append(asyncio.create_task(self._fetch_video_details_batch(batch_ids)))

        # Ex√©cuter les t√¢ches de r√©cup√©ration de lots de mani√®re concurrente
        # return_exceptions=True permet de r√©cup√©rer les r√©sultats ou les exceptions
        batch_results: List[Union[List[Dict[str, Any]], Exception]] = await asyncio.gather(*tasks, return_exceptions=True)

        processed_ids: Set[str] = set() # Suivre les IDs trait√©s pour √©viter doublons
        # Traiter les r√©sultats de chaque lot
        for i, result in enumerate(batch_results):
            batch_num = i + 1
            if isinstance(result, QuotaExceededError):
                # Si un lot √©choue √† cause du quota, arr√™ter tout
                logger.critical(f"Quota d√©pass√© lors de la r√©cup√©ration du lot de d√©tails {batch_num}.")
                raise result # Propager imm√©diatement
            elif isinstance(result, Exception):
                # Logger l'erreur pour ce lot mais continuer avec les autres lots r√©ussis
                logger.error(f"Erreur lors de la r√©cup√©ration du lot de d√©tails {batch_num}: {result}", exc_info=isinstance(result, HttpError))
            elif isinstance(result, list):
                # Le lot a √©t√© r√©cup√©r√© avec succ√®s
                logger.debug(f"Lot de d√©tails {batch_num}: Re√ßu {len(result)} √©l√©ments vid√©o bruts.")
                # Traiter chaque vid√©o dans le lot
                for item in result:
                    video_id = item.get('id')
                    # Ignorer si pas d'ID ou d√©j√† trait√© (s√©curit√©)
                    if not video_id or video_id in processed_ids: continue

                    # Effectuer la validation finale bas√©e sur les d√©tails r√©cup√©r√©s
                    duration_iso = item.get('contentDetails', {}).get('duration')
                    live_status = item.get('snippet', {}).get('liveBroadcastContent', 'none')

                    if self._is_valid_video(duration_iso, live_status):
                        # La vid√©o est valide, cr√©er l'objet Video et l'ajouter
                        video_obj = Video.from_api_response(item)
                        valid_videos_with_details.append(video_obj)
                        processed_ids.add(video_id)
                        logger.log(SPAM_LEVEL_NUM, f"Vid√©o {video_id} valid√©e apr√®s r√©cup√©ration des d√©tails.")
                    else:
                        # La vid√©o n'est pas valide selon les crit√®res finaux
                        logger.log(SPAM_LEVEL_NUM, f"Vid√©o {video_id} filtr√©e apr√®s r√©cup√©ration des d√©tails (dur√©e/statut live).")
            else:
                 # Cas inattendu o√π le r√©sultat n'est ni une liste ni une exception
                 logger.error(f"Type de r√©sultat inattendu pour le lot de d√©tails {batch_num}: {type(result)}")

        logger.info(f"R√©cup√©ration/filtrage des d√©tails termin√©. {len(valid_videos_with_details)} vid√©os valides retenues sur {len(unique_ids)} IDs uniques.")
        return valid_videos_with_details

    async def _fetch_video_details_batch(self, batch_ids: List[str]) -> List[Dict[str, Any]]:
        """R√©cup√®re les d√©tails pour un seul lot d'IDs de vid√©os."""
        if not batch_ids: return []
        ids_string = ",".join(batch_ids)
        logger.debug(f"Appel API videos.list asynchrone pour lot de d√©tails ({len(batch_ids)} IDs)")
        try:
            # Demander les champs n√©cessaires pour cr√©er l'objet Video et pour la validation
            fields="items(id,snippet(title,description,channelId,channelTitle,publishedAt,defaultLanguage,defaultAudioLanguage,tags,liveBroadcastContent),contentDetails/duration)"
            req = self.youtube.videos().list(
                part="snippet,contentDetails",
                id=ids_string,
                fields=fields,
                maxResults=len(batch_ids) # Indiquer le nombre max attendu
            )
            # videos.list a un co√ªt mod√©r√© (environ 5 unit√©s ?)
            resp = await self._execute_api_call(req, cost=5)
            # Retourner la liste des √©l√©ments vid√©o
            return resp.get("items", [])
        except QuotaExceededError:
            logger.critical(f"Quota d√©pass√© pendant la r√©cup√©ration des d√©tails du lot ({len(batch_ids)} IDs)")
            raise
        except HttpError as e:
            # G√©rer les erreurs HTTP pendant la r√©cup√©ration du lot
            logger.error(f"HttpError lors de la r√©cup√©ration du lot de d√©tails ({len(batch_ids)} IDs): {e.resp.status} - {getattr(e, 'uri', 'URI Inconnu')}")
            raise e # Propager pour gestion potentielle en amont
        except Exception as e:
            # G√©rer les erreurs inattendues
            logger.error(f"Erreur inattendue lors de la r√©cup√©ration du lot de d√©tails ({len(batch_ids)} IDs): {e}", exc_info=True)
            raise # Propager

# ==============================================================================
# SECTION 6 : Gestionnaire de Transcriptions
# ==============================================================================

class TranscriptManager:
    """G√®re la r√©cup√©ration, le traitement et la mise en cache des transcriptions vid√©o."""
    # Cache pour les transcriptions finales trait√©es {video_id: Optional[{"language": lang, "transcript": text}]}
    # Contient aussi les r√©sultats n√©gatifs (None) pour √©viter de r√©essayer inutilement
    _final_transcript_cache: Dict[str, Optional[Dict[str, str]]] = {}
    _final_transcript_cache_lock: asyncio.Lock = asyncio.Lock() # Verrou pour ce cache

    # Suivi des erreurs persistantes par ID vid√©o pour √©viter les tentatives r√©p√©t√©es
    _transcript_fetch_errors: Dict[str, Exception] = {}

    # S√©maphore pour limiter le nombre de requ√™tes concurrentes √† l'API de transcription
    _semaphore: asyncio.Semaphore = asyncio.Semaphore(config.TRANSCRIPT_SEMAPHORE_LIMIT)

    @staticmethod
    @functools.lru_cache(maxsize=2048) # Cache pour les lignes de transcription fr√©quemment nettoy√©es
    def _clean_transcript_line(text: str) -> str:
        """Nettoie une seule ligne de texte de transcription."""
        if not text: return ""
        try:
            # Remplacer les sauts de ligne internes par des espaces
            cleaned = text.replace('\n', ' ').replace('\r', ' ')
            # Supprimer les emojis
            cleaned = emoji.replace_emoji(cleaned, replace='')
            # Supprimer les caract√®res de contr√¥le sauf tabulation (sauts de ligne d√©j√† g√©r√©s) et certains caract√®res invisibles/z√©ro-largeur
            cleaned = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F\u200B-\u200D\uFEFF]', '', cleaned)
            # Supprimer les caract√®res Markdown/sp√©ciaux courants
            cleaned = re.sub(r'[\`*_{}\[\]<>|]', '', cleaned)
            # Normaliser les espaces multiples en un seul espace
            return re.sub(r'\s+', ' ', cleaned).strip()
        except Exception:
            # En cas d'erreur (peu probable), retourner le texte original
            logger.warning(f"Erreur lors du nettoyage de la ligne de transcription : '{text[:50]}...'", exc_info=False)
            return text

    def _format_transcript_by_blocks_sync(self, transcript_data: List[Dict[str, Any]], lang_code: str, video_id: str, origin_log: str) -> Optional[str]:
        """
        Formate les donn√©es brutes de transcription (liste de dicts avec 'start', 'text')
        en blocs horodat√©s. S'ex√©cute de mani√®re synchrone.

        Args:
            transcript_data: Liste de dictionnaires [{'start': float, 'text': str, 'duration': float}, ...].
            lang_code: Le code langue de la transcription.
            video_id: L'ID de la vid√©o pour la journalisation.
            origin_log: Cha√Æne d√©crivant l'origine (ex: 'MANUELLE pr√©f√©r√©e') pour les logs.

        Returns:
            La transcription format√©e en cha√Æne de caract√®res, ou None si √©chec.
        """
        if not transcript_data:
            logger.warning(f"Donn√©es de transcription brutes vides fournies (formatage sync) depuis {origin_log} pour {video_id} ({lang_code})")
            return None

        # Dur√©e de chaque bloc de transcription en secondes
        block_duration = float(config.TRANSCRIPT_BLOCK_DURATION_SECONDS)
        if block_duration <= 0:
            logger.error(f"Dur√©e de bloc de transcription invalide configur√©e ({block_duration}s). Formatage impossible.")
            return None

        # Regrouper les segments de texte nettoy√©s par index de bloc temporel
        blocks: Dict[int, List[str]] = {}
        max_block_index = -1 # Suivre le dernier bloc ayant du contenu
        valid_segments_found = False # Drapeau pour v√©rifier si au moins un segment est valide

        for entry in transcript_data:
            segment_start_time = entry.get('start')
            segment_text = entry.get('text')

            # Valider les types des donn√©es du segment
            if not isinstance(segment_start_time, (int, float)) or not isinstance(segment_text, str):
                logger.log(SPAM_LEVEL_NUM, f"Segment ignor√© (formatage sync) √† cause de types invalides : start={type(segment_start_time)}, text={type(segment_text)}")
                continue

            # Nettoyer le texte du segment
            cleaned_segment_text = self._clean_transcript_line(segment_text)
            # Ignorer si le nettoyage r√©sulte en une cha√Æne vide
            if not cleaned_segment_text:
                 logger.log(SPAM_LEVEL_NUM, f"Segment ignor√© apr√®s nettoyage (formatage sync) (original='{segment_text}')")
                 continue

            valid_segments_found = True # Marquer qu'on a trouv√© au moins un segment valide
            # Calculer l'index du bloc bas√© sur le temps de d√©but
            block_index = int(segment_start_time // block_duration)
            # Ajouter le texte nettoy√© au bloc correspondant
            if block_index not in blocks: blocks[block_index] = []
            blocks[block_index].append(cleaned_segment_text)
            # Mettre √† jour l'index du dernier bloc
            max_block_index = max(max_block_index, block_index)

        # Si aucun segment valide n'a √©t√© trouv√© apr√®s nettoyage
        if not valid_segments_found:
            logger.warning(f"Aucun segment de transcription valide trouv√© apr√®s nettoyage (formatage sync) depuis {origin_log} pour {video_id} ({lang_code})")
            return None

        # Assembler la cha√Æne de caract√®res finale format√©e
        formatted_lines: List[str] = []
        # It√©rer sur tous les blocs possibles jusqu'au dernier trouv√© pour conserver la chronologie
        # et repr√©senter implicitement les silences (blocs sans texte)
        for current_index in range(max_block_index + 1):
            block_start_seconds = float(current_index * block_duration)
            timestamp_str = _format_timestamp(block_start_seconds) # Formater le timestamp du d√©but de bloc
            # Si le bloc contient du texte
            if current_index in blocks:
                # Joindre tous les segments de texte du bloc avec un espace
                full_block_text = " ".join(blocks[current_index])
                formatted_lines.append(f"[{timestamp_str}] {full_block_text}")
            # Optionnel : Ajouter une ligne vide ou un marqueur pour les blocs sans texte
            # else:
            #    formatted_lines.append(f"[{timestamp_str}]") # Ligne avec juste le timestamp

        # V√©rifier si le formatage a produit des lignes (ne devrait pas arriver si valid_segments_found est True)
        if not formatted_lines:
            logger.warning(f"Transcription format√©e est vide (formatage sync) malgr√© segments valides depuis {origin_log} pour {video_id} ({lang_code})")
            return None

        # Joindre toutes les lignes format√©es avec un saut de ligne
        full_transcript = "\n".join(formatted_lines)
        logger.debug(f"Transcription {origin_log} format√©e OK (blocs sync {block_duration}s) pour {video_id} ({lang_code}), {len(formatted_lines)} blocs g√©n√©r√©s.")
        return full_transcript

    @functools.lru_cache(maxsize=config.TRANSCRIPT_CACHE_SIZE)
    def _list_transcripts_sync(self, video_id: str) -> Tuple[List[Transcript], Optional[Exception]]:
        """
        Liste de mani√®re synchrone les transcriptions disponibles pour un ID vid√©o.
        Utilise un cache LRU.

        Args:
            video_id: L'ID de la vid√©o.

        Returns:
            Un tuple contenant :
            - Une liste d'objets Transcript disponibles.
            - Une Exception si une erreur s'est produite (y compris non-disponibilit√©), ou None si succ√®s.
        """
        logger.debug(f"Appel synchrone list_transcripts pour {video_id}")
        try:
            # YouTubeTranscriptApi.list_transcripts retourne un g√©n√©rateur, le convertir en liste pour le cache
            transcript_list = list(YouTubeTranscriptApi.list_transcripts(video_id))
            return transcript_list, None # Succ√®s
        except (TranscriptsDisabled, NoTranscriptFound) + YouTubeTranscriptApiErrors.TRANSCRIPT_FETCH_ERRORS as e:
            # Erreurs attendues indiquant la non-disponibilit√©
            logger.warning(f"{e.__class__.__name__} rencontr√©e pour {video_id} (liste sync).")
            return [], e # Retourner liste vide et l'exception sp√©cifique
        except Exception as e:
            # Erreurs inattendues lors du listage
            logger.error(f"Erreur inattendue list_transcripts {video_id} (sync): {e}", exc_info=True)
            return [], e # Retourner liste vide et l'exception g√©n√©rique

    def _fetch_transcript_sync(self, transcript: Transcript) -> Tuple[Optional[List[Dict[str, Any]]], Optional[Exception]]:
        """
        R√©cup√®re de mani√®re synchrone le contenu d'un objet Transcript sp√©cifique.

        Args:
            transcript: L'objet Transcript √† r√©cup√©rer.

        Returns:
            Un tuple contenant :
            - Les donn√©es brutes sous forme de liste de dictionnaires, ou None si √©chec.
            - Une Exception si une erreur s'est produite, ou None si succ√®s.
        """
        video_id = transcript.video_id
        lang_code = transcript.language_code
        logger.debug(f"Appel synchrone fetch_transcript pour {video_id} ({lang_code})")
        try:
            # transcript.fetch() retourne un it√©rable d'objets FetchedTranscriptSnippet
            fetched_snippets = transcript.fetch()

            # Convertir les snippets en une liste de dictionnaires standardis√©s
            transcript_data_list: List[Dict[str, Any]] = []
            for snippet in fetched_snippets:
                # Utiliser getattr pour acc√©der aux attributs de mani√®re s√ªre
                start_time = getattr(snippet, 'start', None)
                duration = getattr(snippet, 'duration', None) # La dur√©e peut √™tre None
                text_content = getattr(snippet, 'text', None)

                # Validation basique des types des champs essentiels
                if isinstance(start_time, (int, float)) and isinstance(text_content, str):
                    transcript_data_list.append({
                        'start': start_time,
                        'duration': duration, # Conserver m√™me si None
                        'text': text_content
                    })
                else:
                    # Logguer les snippets mal form√©s
                    logger.warning(f"Snippet de transcription mal form√© ignor√© pour {video_id} ({lang_code}): start={start_time}, text={text_content}")

            # V√©rifier si des snippets valides ont √©t√© trouv√©s
            if not transcript_data_list:
                 logger.warning(f"La r√©cup√©ration n'a retourn√© aucun snippet valide pour {video_id} ({lang_code})")
                 # Retourner une liste vide, le formateur g√©rera ce cas
                 return [], None

            # Succ√®s, retourner la liste des donn√©es
            return transcript_data_list, None

        except CouldNotRetrieveTranscript as e:
             # Erreur sp√©cifique de l'API lors de la r√©cup√©ration
             logger.error(f"Impossible de r√©cup√©rer le contenu de la transcription {video_id} ({lang_code}) (sync): {e}", exc_info=False)
             return None, e # Retourner None pour les donn√©es et l'exception
        except Exception as e:
             # Erreurs inattendues pendant la r√©cup√©ration
             logger.error(f"Erreur inattendue fetch transcript {video_id} ({lang_code}) (sync): {e}", exc_info=True)
             return None, e # Retourner None et l'exception

    def _select_best_transcript(self,
                                transcripts: List[Transcript],
                                video_id: str,
                                default_language: Optional[str],
                                default_audio_language: Optional[str]
                               ) -> Tuple[Optional[Transcript], str]:
        """
        S√©lectionne la meilleure transcription disponible selon un ordre de pr√©f√©rence.

        Args:
            transcripts: Liste des objets Transcript disponibles pour la vid√©o.
            video_id: ID de la vid√©o (pour logs).
            default_language: Langue par d√©faut des m√©tadonn√©es de la vid√©o.
            default_audio_language: Langue audio par d√©faut de la vid√©o.

        Returns:
            Un tuple contenant :
            - L'objet Transcript s√©lectionn√©, ou None si aucune n'est adapt√©e.
            - Une cha√Æne d√©crivant l'origine de la s√©lection (pour logs).
        """
        if not transcripts:
            return None, "aucune disponible" # Cas simple : pas de transcriptions list√©es

        # Construire l'ordre de pr√©f√©rence des langues :
        # 1. Langue audio par d√©faut (base)
        # 2. Langue des m√©tadonn√©es par d√©faut (base)
        # 3. Langues configur√©es dans config.TRANSCRIPT_LANGUAGES
        pref_langs_base = [lang.split('-')[0] for lang in [default_audio_language, default_language] if lang] # Prendre la base (ex: 'fr' de 'fr-FR')
        # Utiliser dict.fromkeys pour d√©doublonner tout en pr√©servant l'ordre
        pref_langs = list(dict.fromkeys(pref_langs_base + list(config.TRANSCRIPT_LANGUAGES)))
        logger.debug(f"Ordre de pr√©f√©rence des langues pour {video_id}: {pref_langs}")

        # S√©parer les transcriptions manuelles et g√©n√©r√©es
        available_transcripts = {t.language_code: t for t in transcripts}
        available_manual = {lc: t for lc, t in available_transcripts.items() if not t.is_generated}
        available_generated = {lc: t for lc, t in available_transcripts.items() if t.is_generated}

        # Fonction helper pour chercher une langue (correspondance exacte puis langue de base)
        def find_lang(lang_code: str, source_dict: Dict[str, Transcript]) -> Optional[Transcript]:
            """Cherche une langue dans un dictionnaire de transcriptions."""
            # Chercher correspondance exacte (ex: 'fr-FR')
            if lang_code in source_dict: return source_dict[lang_code]
            # Si pas trouv√©, chercher la langue de base (ex: 'fr')
            base_lang = lang_code.split('-')[0]
            # V√©rifier la langue de base seulement si elle est diff√©rente du code original
            if base_lang != lang_code and base_lang in source_dict: return source_dict[base_lang]
            # Non trouv√©
            return None

        # Logique de s√©lection par priorit√© :
        # 1. Chercher dans les langues pr√©f√©r√©es parmi les transcriptions MANUELLES
        for lang in pref_langs:
            transcript = find_lang(lang, available_manual)
            if transcript: return transcript, "MANUELLE pr√©f√©r√©e"
        # 2. Chercher dans les langues pr√©f√©r√©es parmi les transcriptions G√âN√âR√âES
        for lang in pref_langs:
            transcript = find_lang(lang, available_generated)
            if transcript: return transcript, "G√âN√âR√âE pr√©f√©r√©e"
        # 3. Si toujours pas trouv√©, prendre N'IMPORTE QUELLE transcription MANUELLE (repli)
        if available_manual:
            # Retourner la premi√®re trouv√©e (l'ordre peut varier mais garantit une manuelle si existe)
            first_manual = next(iter(available_manual.values()))
            return first_manual, "MANUELLE (repli)"
        # 4. Si toujours pas trouv√©, prendre N'IMPORTE QUELLE transcription G√âN√âR√âE (dernier recours)
        if available_generated:
            first_generated = next(iter(available_generated.values()))
            return first_generated, "G√âN√âR√âE (repli)"

        # Si on arrive ici, c'est qu'il n'y avait aucune transcription exploitable
        return None, "aucune trouv√©e correspondant aux crit√®res"

    async def get_transcript(self, video_id: str, default_language: Optional[str] = None, default_audio_language: Optional[str] = None) -> Optional[Dict[str, str]]:
        """
        R√©cup√®re de mani√®re asynchrone la meilleure transcription disponible pour un ID vid√©o,
        la formate en blocs horodat√©s et met en cache le r√©sultat (positif ou n√©gatif).

        Args:
            video_id: L'ID de la vid√©o YouTube.
            default_language: Langue par d√©faut des m√©tadonn√©es (aide √† la s√©lection).
            default_audio_language: Langue audio par d√©faut (aide √† la s√©lection).

        Returns:
            Un dictionnaire {"language": code, "transcript": texte_format√©} ou None si √©chec/non disponible.
        """
        # V√©rifier le cache final en premier (contient les succ√®s et les √©checs connus)
        async with self._final_transcript_cache_lock:
            if video_id in self._final_transcript_cache:
                logger.spam(f"Cache final HIT pour transcription {video_id}")
                return self._final_transcript_cache[video_id] # Peut √™tre None si √©chec pr√©c√©dent mis en cache
            # V√©rifier aussi s'il y a une erreur persistante connue pour cet ID
            if video_id in self._transcript_fetch_errors:
                 logger.warning(f"R√©cup√©ration transcription ignor√©e pour {video_id} √† cause d'une erreur persistante connue : {self._transcript_fetch_errors[video_id]}")
                 return None # Ne pas r√©essayer

        # Acqu√©rir le s√©maphore pour limiter les requ√™tes concurrentes
        async with self._semaphore:
            logger.info(f"Tentative de r√©cup√©ration asynchrone de transcription pour {video_id}")

            # --- √âtape 1: Lister les transcriptions disponibles (via appel sync mis en cache) ---
            try:
                transcripts, list_error = await asyncio.to_thread(self._list_transcripts_sync, video_id)
                if list_error:
                    # G√©rer les erreurs attendues o√π les transcriptions ne sont simplement pas disponibles
                    if isinstance(list_error, (TranscriptsDisabled, NoTranscriptFound) + YouTubeTranscriptApiErrors.TRANSCRIPT_FETCH_ERRORS):
                        logger.warning(f"Transcriptions non disponibles pour {video_id}: {list_error.__class__.__name__}")
                        # Mettre en cache le r√©sultat n√©gatif et l'erreur persistante
                        async with self._final_transcript_cache_lock:
                            self._final_transcript_cache[video_id] = None
                            self._transcript_fetch_errors[video_id] = list_error
                        return None
                    else:
                        # Remonter les erreurs inattendues lors du listage
                        raise list_error
            except Exception as e:
                 # G√©rer les erreurs inattendues pendant l'appel asynchrone √† list_transcripts
                 logger.error(f"Erreur lors de l'appel asynchrone √† list_transcripts pour {video_id}: {e}", exc_info=True)
                 # Mettre en cache le r√©sultat n√©gatif en cas d'erreur de listage inattendue
                 async with self._final_transcript_cache_lock: self._final_transcript_cache[video_id] = None
                 return None

            # --- √âtape 2: S√©lectionner la meilleure transcription parmi celles disponibles ---
            target_transcript, log_origin = self._select_best_transcript(
                transcripts, video_id, default_language, default_audio_language
            )

            # Si aucune transcription appropri√©e n'a √©t√© trouv√©e
            if not target_transcript:
                logger.warning(f"Aucune transcription adapt√©e trouv√©e pour {video_id} selon les pr√©f√©rences ({log_origin}).")
                # Mettre en cache le r√©sultat n√©gatif
                async with self._final_transcript_cache_lock: self._final_transcript_cache[video_id] = None
                return None

            # --- √âtape 3: R√©cup√©rer le contenu de la transcription s√©lectionn√©e ---
            logger.debug(f"Tentative de r√©cup√©ration asynchrone pour {video_id} ({target_transcript.language_code}) - Origine: {log_origin}")
            try:
                # Ex√©cuter l'appel de r√©cup√©ration synchrone dans un thread
                transcript_data, fetch_error = await asyncio.to_thread(self._fetch_transcript_sync, target_transcript)

                # G√©rer les erreurs de r√©cup√©ration
                if fetch_error:
                    logger.warning(f"√âchec de la r√©cup√©ration de la transcription {log_origin} pour {video_id} ({target_transcript.language_code}): {fetch_error}")
                    # Mettre en cache l'erreur si elle est probablement persistante
                    if isinstance(fetch_error, CouldNotRetrieveTranscript):
                         async with self._final_transcript_cache_lock:
                             # Ne pas √©craser une erreur pr√©c√©dente si elle existe d√©j√†
                             if video_id not in self._transcript_fetch_errors:
                                 self._transcript_fetch_errors[video_id] = fetch_error
                    # Ne pas mettre en cache de r√©sultat n√©gatif ici, l'erreur pourrait √™tre temporaire
                    return None # Indiquer l'√©chec pour cette tentative

                # --- √âtape 4: Formater la transcription r√©cup√©r√©e ---
                if transcript_data is not None: # Peut √™tre une liste vide si fetch OK mais pas de contenu
                    # Ex√©cuter le formatage synchrone dans un thread
                    formatted_text = await asyncio.to_thread(
                        self._format_transcript_by_blocks_sync,
                        transcript_data, target_transcript.language_code, video_id, log_origin
                    )

                    if formatted_text is not None:
                        # Succ√®s ! Mettre en cache et retourner le r√©sultat.
                        logger.info(f"Transcription r√©cup√©r√©e et format√©e avec succ√®s pour {video_id} ({target_transcript.language_code}) - Origine: {log_origin}")
                        result = {"language": target_transcript.language_code, "transcript": formatted_text}
                        async with self._final_transcript_cache_lock:
                            self._final_transcript_cache[video_id] = result
                        return result
                    else:
                        # Le formatage a √©chou√© ou a r√©sult√© en un texte vide
                        logger.warning(f"Le formatage a √©chou√© ou a r√©sult√© en une transcription vide pour {video_id} ({target_transcript.language_code})")
                        # Mettre en cache un r√©sultat n√©gatif si le formatage √©choue de mani√®re constante
                        async with self._final_transcript_cache_lock:
                            self._final_transcript_cache[video_id] = None
                        return None
                else:
                    # Cas o√π fetch_error √©tait None, mais transcript_data est None (ne devrait pas arriver)
                     logger.warning(f"La r√©cup√©ration de la transcription a retourn√© None sans erreur pour {video_id} ({target_transcript.language_code})")
                     async with self._final_transcript_cache_lock:
                         self._final_transcript_cache[video_id] = None
                     return None

            except Exception as e:
                 # Capturer les erreurs inattendues pendant le processus de r√©cup√©ration/formatage
                 logger.error(f"Erreur inattendue pendant le processus get_transcript pour {video_id} ({target_transcript.language_code}): {e}", exc_info=True)
                 # Mettre en cache un r√©sultat n√©gatif en cas d'erreur inattendue
                 async with self._final_transcript_cache_lock:
                     self._final_transcript_cache[video_id] = None
                 return None

        # Point de sortie si le s√©maphore expire ou autre √©chec non g√©r√©
        logger.error(f"Le traitement de la transcription a √©chou√© de mani√®re inattendue pour {video_id} apr√®s le s√©maphore.")
        async with self._final_transcript_cache_lock:
            self._final_transcript_cache[video_id] = None
        return None

# ==============================================================================
# SECTION 7 : Gestionnaire de Fichiers
# ==============================================================================

# Tentative de chargement global de l'encodeur Tiktoken
ENCODING: Optional[tiktoken.Encoding] = None
try:
    ENCODING = tiktoken.get_encoding("cl100k_base")
    logger.debug("Encodeur Tiktoken cl100k_base charg√© avec succ√®s.")
except Exception as e:
    logger.warning(f"Impossible de charger l'encodeur Tiktoken : {e}. Le comptage de tokens pour le d√©coupage de fichiers sera d√©sactiv√©.")

def save_video_data_files(
    videos: List[Video],
    source_name: str,
    output_format: str, # 'txt', 'md', 'yaml'
    include_description: bool = True,
    progress_callback: Optional[Callable[[int, int], None]] = None
) -> List[Tuple[Path, int, int]]:
    """
    Sauvegarde les donn√©es vid√©o (m√©tadonn√©es, description, transcription) dans des fichiers
    au format sp√©cifi√© (txt, md, yaml), en les d√©coupant en parties si n√©cessaire
    en fonction du nombre estim√© de tokens.

    Args:
        videos: Liste d'objets Video √† sauvegarder (suppos√©e tri√©e).
        source_name: Nom descriptif de la source (ex: titre playlist) utilis√© pour les noms de fichiers.
        output_format: Le format de sortie souhait√© ('txt', 'md', 'yaml').
        include_description: Inclure la description de la vid√©o dans la sortie.
        progress_callback: Fonction optionnelle pour rapporter la progression (index_video_courante, total_videos).

    Returns:
        Une liste de tuples, chaque tuple contenant (Chemin_fichier_cr√©√©, tokens_estim√©s, nombre_videos).
    """
    if not videos:
        logger.info("Aucune vid√©o fournie pour la sauvegarde.")
        return []

    logger.info(f"D√©but du processus de sauvegarde pour la source '{source_name}' ({len(videos)} vid√©os) au format : {output_format.upper()}")

    created_files: List[Tuple[Path, int, int]] = [] # Liste pour stocker les infos des fichiers cr√©√©s
    current_part_number = 1 # Num√©ro de la partie en cours
    # Stocker les donn√©es format√©es (cha√Æne pour txt/md, dict pour yaml)
    current_part_data: Union[List[str], List[Dict[str, Any]]]
    current_part_tokens = 0 # Tokens estim√©s pour la partie en cours
    current_part_video_count = 0 # Nombre de vid√©os dans la partie en cours
    total_videos_processed = 0 # Compteur global pour le callback de progression

    # G√©n√©rer un nom de fichier de base s√ªr √† partir du nom de la source
    base_filename = clean_title(source_name) or f"Resultats_{datetime.now():%Y%m%d_%H%M%S}"
    save_folder = config.VIDEOS_FOLDER # Dossier de sauvegarde configur√©
    file_extension = f".{output_format}" # Extension bas√©e sur le format

    # D√©terminer la fonction de formatage et initialiser la liste de donn√©es de la partie
    formatter: Callable[[Video, bool], Union[str, Dict[str, Any]]]
    if output_format == 'yaml':
        formatter = lambda v, inc_desc: v.to_dict(inc_desc)
        current_part_data = [] # Liste de dictionnaires pour YAML
    elif output_format == 'md':
        formatter = lambda v, inc_desc: v.to_markdown(inc_desc)
        current_part_data = [] # Liste de cha√Ænes Markdown
    else: # Par d√©faut : txt
        formatter = lambda v, inc_desc: v.to_texte(inc_desc)
        current_part_data = [] # Liste de cha√Ænes texte

    # It√©rer sur chaque vid√©o √† sauvegarder
    for idx, video in enumerate(videos):
        logger.debug(f"Pr√©paration de la vid√©o {idx+1}/{len(videos)} ({video.id}) pour sauvegarde en {output_format.upper()}.")
        try:
            # Formater les donn√©es de la vid√©o selon le format choisi
            formatted_data: Union[str, Dict[str, Any]] = formatter(video, include_description)
            item_tokens = 0 # Tokens estim√©s pour cette vid√©o
            item_str_representation = "" # Repr√©sentation cha√Æne pour l'estimation des tokens

            # Obtenir la repr√©sentation cha√Æne pour l'estimation
            if isinstance(formatted_data, str):
                item_str_representation = formatted_data
            elif isinstance(formatted_data, dict):
                # Pour YAML, estimer les tokens en dumpant temporairement l'item unique
                try:
                    # Utiliser des options pour un dump YAML lisible
                    item_str_representation = yaml.dump(
                        formatted_data, allow_unicode=True, sort_keys=False, width=120, default_flow_style=False
                    )
                except Exception as yaml_err:
                    logger.warning(f"Impossible de dumper le dict vid√©o en cha√Æne pour l'estimation des tokens ({video.id}): {yaml_err}")
                    item_str_representation = str(formatted_data) # Solution de repli

            # Estimer les tokens si l'encodeur est disponible et une limite est d√©finie
            if ENCODING and config.MAX_TOKENS_PER_FILE > 0 and item_str_representation:
                try:
                    item_tokens = len(ENCODING.encode(item_str_representation))
                except Exception as enc_e:
                    logger.warning(f"√âchec de l'encodage Tiktoken pour la repr√©sentation de la vid√©o {video.id}: {enc_e}. Tokens compt√©s comme 0.")

            # V√©rifier si l'ajout de cette vid√©o d√©passe la limite de tokens pour la partie en cours
            # D√©couper seulement si une limite est d√©finie (> 0) et si la partie contient d√©j√† des vid√©os
            if (current_part_video_count > 0 and
                config.MAX_TOKENS_PER_FILE > 0 and
                current_part_tokens + item_tokens > config.MAX_TOKENS_PER_FILE):

                # Sauvegarder la partie actuelle avant d'en commencer une nouvelle
                part_filename = f"{base_filename}_partie-{current_part_number:02d}{file_extension}"
                part_filepath = save_folder / part_filename
                logger.debug(f"Limite de tokens ({config.MAX_TOKENS_PER_FILE}) atteinte. Sauvegarde de la partie {current_part_number} ({current_part_tokens} tokens, {current_part_video_count} vid√©os) -> {part_filepath.name}")

                # Appeler la fonction de sauvegarde appropri√©e selon le format
                save_successful = False
                if output_format == 'yaml':
                    # Assurer que current_part_data est bien une liste de dicts
                    if all(isinstance(item, dict) for item in current_part_data):
                        save_successful = _save_yaml_content(part_filepath, current_part_data)
                    else:
                        logger.error(f"Type de donn√©es incorrect pour la sauvegarde YAML de la partie {current_part_number}.")
                else: # txt or md
                    # Assurer que current_part_data est bien une liste de str
                    if all(isinstance(item, str) for item in current_part_data):
                        save_successful = _save_formatted_text(part_filepath, current_part_data, output_format)
                    else:
                         logger.error(f"Type de donn√©es incorrect pour la sauvegarde {output_format.upper()} de la partie {current_part_number}.")

                # Enregistrer les informations du fichier si la sauvegarde a r√©ussi
                if save_successful:
                    created_files.append((part_filepath, current_part_tokens, current_part_video_count))
                else:
                    logger.error(f"√âchec de la sauvegarde de la partie {current_part_number} vers {part_filepath.name}")

                # R√©initialiser pour la nouvelle partie
                current_part_number += 1
                # R√©initialiser la liste de donn√©es en fonction du format
                if output_format == 'yaml': current_part_data = []
                else: current_part_data = []
                current_part_tokens = 0
                current_part_video_count = 0

            # Ajouter les donn√©es format√©es de la vid√©o actuelle √† la partie en cours
            # Type checker peut avoir du mal ici, mais la logique est correcte
            current_part_data.append(formatted_data) # type: ignore
            current_part_tokens += item_tokens
            current_part_video_count += 1
            total_videos_processed += 1

            # Rapporter la progression via le callback
            if progress_callback:
                progress_callback(total_videos_processed, len(videos))

        except Exception as e:
            # G√©rer les erreurs lors du traitement d'une vid√©o sp√©cifique
            logger.error(f"Erreur lors du traitement de la vid√©o {video.id} pour la sauvegarde: {e}", exc_info=True)
            # Continuer avec la vid√©o suivante

    # Sauvegarder le contenu restant dans la derni√®re partie
    if current_part_data:
        # D√©terminer si c'est la seule partie (pas de suffixe _partie-XX)
        is_single_part = (current_part_number == 1 and not created_files)
        final_filename = f"{base_filename}{file_extension}" if is_single_part else f"{base_filename}_partie-{current_part_number:02d}{file_extension}"
        final_filepath = save_folder / final_filename
        part_desc = "unique" if is_single_part else f"{current_part_number} (finale)"
        logger.debug(f"Sauvegarde de la partie {part_desc} ({current_part_tokens} tokens, {current_part_video_count} vid√©os) -> {final_filepath.name}")

        # Appeler la fonction de sauvegarde appropri√©e
        save_successful = False
        if output_format == 'yaml':
            if all(isinstance(item, dict) for item in current_part_data):
                save_successful = _save_yaml_content(final_filepath, current_part_data) # type: ignore
            else: logger.error("Type de donn√©es incorrect pour sauvegarde YAML finale.")
        else: # txt or md
            if all(isinstance(item, str) for item in current_part_data):
                save_successful = _save_formatted_text(final_filepath, current_part_data, output_format) # type: ignore
            else: logger.error(f"Type de donn√©es incorrect pour sauvegarde {output_format.upper()} finale.")

        # Enregistrer les informations du fichier si succ√®s
        if save_successful:
            created_files.append((final_filepath, current_part_tokens, current_part_video_count))
        else:
            logger.error(f"√âchec de la sauvegarde de la partie {part_desc} vers {final_filepath.name}")

    logger.info(f"Sauvegarde termin√©e pour la source '{source_name}'. Cr√©√© {len(created_files)} fichier(s) dans '{save_folder.relative_to(config.WORK_FOLDER)}'.")
    return created_files

def _save_formatted_text(file_path: Path, content_list: List[str], format_type: str) -> bool:
    """Sauvegarde une liste de cha√Ænes (TXT ou MD) dans un fichier."""
    if not content_list:
        logger.warning(f"Tentative de sauvegarde de contenu texte vide vers {file_path}. Ignor√©.")
        return False
    try:
        logger.debug(f"√âcriture de {len(content_list)} blocs de contenu dans le fichier {format_type.upper()} : {file_path.name}")
        # Choisir le s√©parateur en fonction du format
        separator = "\n\n---\n\n" if format_type == 'md' else "\n\n" + "=" * 80 + "\n\n"
        # Utiliser newline='' pour √©viter les doubles sauts de ligne potentiels sous Windows
        with file_path.open("w", encoding="utf-8", errors="replace", newline='') as f:
            f.write(separator.join(content_list))
        logger.debug(f"Fichier {format_type.upper()} √©crit avec succ√®s : {file_path.name}")
        return True
    except OSError as e:
        logger.error(f"Erreur OS lors de l'√©criture du fichier {file_path}: {e}")
        return False
    except Exception as e:
        logger.error(f"Erreur inattendue lors de l'√©criture du fichier {file_path}: {e}", exc_info=True)
        return False

def _save_yaml_content(file_path: Path, data_list: List[Dict[str, Any]]) -> bool:
    """Sauvegarde une liste de dictionnaires dans un fichier YAML."""
    if not data_list:
        logger.warning(f"Tentative de sauvegarde d'une liste de donn√©es vide vers le fichier YAML {file_path}. Ignor√©.")
        return False
    try:
        logger.debug(f"√âcriture de {len(data_list)} entr√©es de donn√©es vid√©o dans le fichier YAML : {file_path.name}")
        # Utiliser PyYAML pour √©crire la liste de dictionnaires
        with file_path.open("w", encoding="utf-8") as f:
            # allow_unicode=True pour pr√©server les caract√®res non-ASCII
            # sort_keys=False pour garder l'ordre des cl√©s d√©fini dans to_dict
            # width=120 pour un formatage plus large
            # default_flow_style=False pour un style bloc (plus lisible)
            yaml.dump(data_list, f, allow_unicode=True, sort_keys=False, width=120, default_flow_style=False)
        logger.debug(f"Fichier YAML √©crit avec succ√®s : {file_path.name}")
        return True
    except yaml.YAMLError as e:
        # Erreur sp√©cifique √† YAML
        logger.error(f"Erreur YAML lors de l'√©criture du fichier {file_path}: {e}")
        return False
    except OSError as e:
        # Erreur li√©e au syst√®me de fichiers
        logger.error(f"Erreur OS lors de l'√©criture du fichier {file_path}: {e}")
        return False
    except Exception as e:
        # Autres erreurs inattendues
        logger.error(f"Erreur inattendue lors de l'√©criture du fichier {file_path}: {e}", exc_info=True)
        return False

def read_urls_from_file(file_path_str: str) -> List[str]:
    """Lit les URLs depuis un fichier texte, ignorant les lignes vides et les commentaires (#)."""
    path = Path(file_path_str)
    logger.info(f"Lecture des URLs depuis le fichier : {path.resolve()}")
    if not path.is_file():
        logger.error(f"Fichier d'URLs non trouv√© : {path.resolve()}")
        return [] # Retourner liste vide si fichier non trouv√©

    urls: List[str] = []
    try:
        # Sp√©cifier explicitement l'encodage utf-8
        with path.open('r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                stripped_line = line.strip()
                # Ajouter si la ligne n'est pas vide et ne commence pas par #
                if stripped_line and not stripped_line.startswith('#'):
                    urls.append(stripped_line)
                elif stripped_line.startswith('#'):
                     # Logguer les commentaires ignor√©s au niveau SPAM
                     logger.log(SPAM_LEVEL_NUM, f"Ligne {line_num} ignor√©e (commentaire) dans {path.name}")
        logger.info(f"{len(urls)} URLs lues depuis {path.name}")
        return urls
    except Exception as e:
        # G√©rer les erreurs de lecture de fichier
        logger.error(f"Erreur lors de la lecture du fichier d'URLs {path.resolve()}: {e}", exc_info=True)
        return [] # Retourner liste vide en cas d'erreur

# ==============================================================================
# SECTION 8 : Moteur du Scraper
# ==============================================================================

class YoutubeScraperEngine:
    """Orchestre le processus de r√©cup√©ration et de sauvegarde des donn√©es YouTube pour plusieurs URLs."""
    TOTAL_STEPS: int = 5 # Nombre total d'√©tapes pour l'affichage de la progression

    def __init__(self, api_client: YouTubeAPIClient, transcript_manager: TranscriptManager, progress_display: ProgressDisplay):
        """
        Initialise le moteur du scraper.

        Args:
            api_client: Instance du client API YouTube.
            transcript_manager: Instance du gestionnaire de transcriptions.
            progress_display: Instance du gestionnaire d'affichage de la progression.
        """
        self.api_client: YouTubeAPIClient = api_client
        self.transcript_manager: TranscriptManager = transcript_manager
        self.progress_display: ProgressDisplay = progress_display
        self.quota_reached: bool = False # Indicateur pour arr√™ter le traitement si le quota est atteint

    async def _mark_steps_as_status(self, url_index: int, start_step: int, message: str, status: str) -> None:
        """Marque toutes les √©tapes √† partir de start_step avec le statut et le message donn√©s."""
        # Utiliser l'ic√¥ne correspondant au statut, ou l'ic√¥ne d'erreur par d√©faut
        icon_status = status if status in ProgressDisplay.ICONS else ProgressStatus.ERROR
        # It√©rer sur les √©tapes restantes (ou toutes si start_step=1)
        for step in range(start_step, self.TOTAL_STEPS + 1):
            # V√©rifier si l'√©tape existe pour cet index d'URL avant de la mettre √† jour
            if url_index in self.progress_display.url_steps_mapping and step in self.progress_display.url_steps_mapping[url_index]:
                 # Utiliser le message principal pour la premi√®re √©tape marqu√©e, un message g√©n√©rique ensuite
                 step_message = message if step == start_step else "√âtape pr√©c√©dente √©chou√©e/arr√™t√©e"
                 await self.progress_display.update_step(url_index, step, icon_status, message=step_message)

    @staticmethod
    def _get_sort_key(video: Video) -> datetime:
        """Retourne la date de publication datetime pour le tri (utilise min datetime si inconnu)."""
        dt = video.get_published_at_datetime()
        # Utiliser datetime.min avec fuseau horaire UTC pour les vid√©os sans date valide
        # pour assurer un tri coh√©rent (g√©n√©ralement au d√©but).
        return dt if dt else datetime.min.replace(tzinfo=timezone.utc)

    # --- M√©thodes priv√©es pour chaque √©tape du traitement d'une URL ---

    async def _process_step_1_analyze(self, url: str, url_index: int) -> Optional[Tuple[str, str]]:
        """Traite l'√âtape 1 : Analyse de l'URL."""
        step = 1
        await self.progress_display.update_step(url_index, step, ProgressStatus.IN_PROGRESS, message="Analyse URL...")
        identifier_info = await self.api_client.extract_identifier(url)
        if not identifier_info:
            raise ValueError("Format d'URL invalide ou non reconnu") # Lev√© pour √™tre attrap√© par process_url
        identifier, content_type = identifier_info
        await self.progress_display.update_step(url_index, step, ProgressStatus.DONE, message=f"Type: {content_type}")
        return identifier, content_type

    async def _process_step_2_get_ids(self, url_index: int, content_type: str, identifier: str) -> Tuple[List[str], str]:
        """Traite l'√âtape 2 : R√©cup√©ration des IDs Vid√©o."""
        step = 2
        await self.progress_display.update_step(url_index, step, ProgressStatus.IN_PROGRESS, message="Recherche vid√©os...")
        video_ids, source_name = await self.api_client.get_videos_from_source(content_type, identifier)
        if not video_ids:
            # Avertir mais ne pas lever d'erreur, retourner liste vide
            await self.progress_display.show_warning(f"0 vid√©o trouv√©e ou valide pour '{source_name}' (filtre initial)")
            await self.progress_display.update_step(url_index, step, ProgressStatus.WARNING, message="0 vid√©o trouv√©e/valide")
            return [], source_name # Retourner liste vide et nom de source
        await self.progress_display.update_step(url_index, step, ProgressStatus.DONE, message=f"{len(video_ids)} vid√©o(s) potentielle(s)")
        return video_ids, source_name

    async def _process_step_3_get_details_transcripts(self, url_index: int, video_ids: List[str], source_name: str, include_transcript: bool) -> Tuple[List[Video], int]:
        """Traite l'√âtape 3 : R√©cup√©ration des D√©tails & Transcriptions."""
        step = 3
        await self.progress_display.update_step(url_index, step, ProgressStatus.IN_PROGRESS, message="R√©cup. d√©tails...")
        videos_details = await self.api_client.get_video_details_batch(video_ids)
        if not videos_details:
            # Avertir mais ne pas lever d'erreur
            await self.progress_display.show_warning(f"Aucune vid√©o valide apr√®s r√©cup√©ration des d√©tails pour '{source_name}'.")
            await self.progress_display.update_step(url_index, step, ProgressStatus.WARNING, message="0 vid√©o valide post-d√©tails")
            return [], 0 # Retourner liste vide

        final_vid_count = len(videos_details)
        transcript_count = 0
        # Traiter les transcriptions seulement si demand√©
        if include_transcript:
            await self.progress_display.update_step(url_index, step, ProgressStatus.IN_PROGRESS, message=f"R√©cup. transcriptions ({final_vid_count} vid√©os)...")
            # Cette m√©thode g√®re sa propre progression interne pour les transcriptions
            videos_details, transcript_count = await self._process_transcripts_concurrently(videos_details, url_index, step)
            final_vid_count = len(videos_details) # Mettre √† jour au cas o√π des erreurs critiques se produisent

        # V√©rifier s'il reste des vid√©os apr√®s le traitement des transcriptions
        if final_vid_count == 0:
             await self.progress_display.show_warning(f"Aucune vid√©o restante apr√®s traitement des transcriptions pour '{source_name}'.")
             await self.progress_display.update_step(url_index, step, ProgressStatus.WARNING, message="0 vid√©o apr√®s transcripts")
             return [], 0

        # Marquer l'√©tape comme termin√©e
        await self.progress_display.update_step(url_index, step, ProgressStatus.DONE, message=f"{final_vid_count} vid√©o(s), {transcript_count} transcript(s)")
        return videos_details, transcript_count

    async def _process_step_4_sort(self, url_index: int, videos_details: List[Video], source_name: str) -> List[Video]:
        """Traite l'√âtape 4 : Tri des Vid√©os."""
        step = 4
        await self.progress_display.update_step(url_index, step, ProgressStatus.SORT, message="Tri chronologique...")
        try:
            # Trier la liste de vid√©os en place par date de publication
            videos_details.sort(key=self._get_sort_key)
            logger.info(f"Tri chronologique termin√© pour {len(videos_details)} vid√©os de '{source_name}'.")
            await self.progress_display.update_step(url_index, step, ProgressStatus.DONE, message=f"{len(videos_details)} tri√©e(s)")
            return videos_details
        except Exception as sort_e:
            # G√©rer les erreurs potentielles pendant le tri
            logger.error(f"Erreur lors du tri des vid√©os pour '{source_name}': {sort_e}", exc_info=True)
            await self.progress_display.update_step(url_index, step, ProgressStatus.ERROR, message=f"Erreur de tri: {sort_e}")
            raise # Remonter l'erreur pour arr√™ter le traitement de cette URL

    async def _process_step_5_save(self, url_index: int, videos_details: List[Video], source_name: str, output_format: str, include_description: bool) -> List[Tuple[Path, int, int]]:
        """Traite l'√âtape 5 : Sauvegarde des Fichiers dans le format sp√©cifi√©."""
        step = 5
        await self.progress_display.update_step(url_index, step, ProgressStatus.IN_PROGRESS, message=f"Sauvegarde ({output_format.upper()})...")

        loop = asyncio.get_running_loop()
        # D√©finir un wrapper de callback synchrone pour mettre √† jour l'affichage async depuis le thread sync
        def progress_callback_sync(current: int, total: int):
            """Callback synchrone pour la progression de la sauvegarde."""
            if total > 0:
                percent = (current / total) * 100
                message = f"{current}/{total}"
                # Planifier l'ex√©cution de la coroutine de mise √† jour dans la boucle d'√©v√©nements principale
                asyncio.run_coroutine_threadsafe(
                    self.progress_display.update_step(url_index, step, ProgressStatus.IN_PROGRESS, percent, message),
                    loop
                )

        # Ex√©cuter la fonction de sauvegarde (potentiellement longue) dans un thread s√©par√©
        created_files = await asyncio.to_thread(
            save_video_data_files, # Utiliser la fonction de sauvegarde mise √† jour
            videos_details,
            source_name,
            output_format, # Passer le format de sortie
            include_description,
            progress_callback_sync # Passer le callback synchrone
        )
        # Marquer l'√©tape comme termin√©e
        await self.progress_display.update_step(url_index, step, ProgressStatus.DONE, message=f"{len(created_files)} fichier(s) ({output_format.upper()})")
        return created_files

    # --- M√©thode principale de traitement d'une URL ---
    async def process_url(self, url: str, url_index: int, output_format: str, include_transcript: bool, include_description: bool) -> List[Tuple[Path, int, int]]:
        """
        Traite une seule URL √† travers toutes les √©tapes : analyse, r√©cup√©ration IDs,
        r√©cup√©ration d√©tails/transcriptions, tri, sauvegarde.
        G√®re les erreurs et les exceptions de quota pour le flux de traitement de cette URL.

        Args:
            url: L'URL YouTube √† traiter.
            url_index: L'index s√©quentiel de cette URL dans la liste globale.
            output_format: Le format de sortie souhait√© ('txt', 'md', 'yaml').
            include_transcript: Faut-il inclure les transcriptions ?
            include_description: Faut-il inclure les descriptions ?

        Returns:
            Une liste des fichiers cr√©√©s pour cette URL, ou une liste vide en cas d'√©chec majeur.
        """
        all_files_for_this_url: List[Tuple[Path, int, int]] = []
        source_name = url # Nom de source par d√©faut
        current_step = 0 # Suivre l'√©tape actuelle pour les logs d'erreur

        try:
            # Afficher l'en-t√™te pour cette URL (retourne l'index r√©el utilis√© par ProgressDisplay)
            actual_url_index = await self.progress_display.show_url_header(url)
            # S'assurer qu'on utilise le bon index pour les mises √† jour suivantes
            if actual_url_index != url_index:
                 logger.warning(f"D√©saccord d'index URL : attendu {url_index}, obtenu {actual_url_index}. Utilisation de {actual_url_index}.")
                 url_index = actual_url_index # Corriger l'index

            # --- Ex√©cution des √©tapes s√©quentielles ---
            current_step = 1
            identifier_info = await self._process_step_1_analyze(url, url_index)
            if not identifier_info: return [] # Erreur g√©r√©e dans la m√©thode step
            identifier, content_type = identifier_info

            current_step = 2
            video_ids, source_name = await self._process_step_2_get_ids(url_index, content_type, identifier)
            if not video_ids:
                # Si aucune vid√©o trouv√©e, marquer les √©tapes restantes comme ignor√©es et retourner
                await self._mark_steps_as_status(url_index, current_step + 1, "Aucune vid√©o trouv√©e/valide", ProgressStatus.SKIPPED)
                return []

            current_step = 3
            videos_details, _ = await self._process_step_3_get_details_transcripts(url_index, video_ids, source_name, include_transcript)
            if not videos_details:
                # Si aucune vid√©o apr√®s d√©tails/transcripts, marquer et retourner
                await self._mark_steps_as_status(url_index, current_step + 1, "Aucune vid√©o apr√®s d√©tails/transcripts", ProgressStatus.SKIPPED)
                return []

            current_step = 4
            # Le tri peut lever une exception, qui sera attrap√©e ci-dessous
            videos_details = await self._process_step_4_sort(url_index, videos_details, source_name)

            current_step = 5
            # La sauvegarde peut aussi √©chouer, mais g√®re ses erreurs internes
            created_files = await self._process_step_5_save(url_index, videos_details, source_name, output_format, include_description)
            all_files_for_this_url.extend(created_files)

        except QuotaExceededError:
            # G√©rer sp√©cifiquement l'erreur de quota
            error_msg = "Quota API YouTube d√©pass√©"
            logger.critical(f"{error_msg} lors du traitement de l'URL {url_index} ({url}) √† l'√©tape {current_step}.")
            await self.progress_display.show_quota_exceeded("Traitement arr√™t√© pour les URLs suivantes.")
            # Marquer les √©tapes restantes comme √©chou√©es √† cause du quota
            await self._mark_steps_as_status(url_index, current_step, error_msg, ProgressStatus.QUOTA)
            self.quota_reached = True # Mettre le drapeau pour arr√™ter le traitement global
        except (ValueError, HttpError) as e:
            # G√©rer les erreurs attendues (URL invalide, ressource non trouv√©e, etc.)
            error_msg = str(e)
            logger.error(f"Erreur lors du traitement de l'URL {url_index} ({url}) √† l'√©tape {current_step}: {error_msg}", exc_info=isinstance(e, HttpError))
            await self.progress_display.show_error(f"Erreur: {error_msg}")
            # Marquer les √©tapes restantes comme √©chou√©es
            await self._mark_steps_as_status(url_index, current_step, error_msg, ProgressStatus.ERROR)
        except Exception as e_global:
            # G√©rer les erreurs critiques inattendues
            error_msg = f"Erreur inattendue: {e_global}"
            logger.critical(f"Erreur critique lors du traitement de l'URL {url_index} ({url}) √† l'√©tape {current_step}: {error_msg}", exc_info=True)
            await self.progress_display.show_error(f"Erreur Critique: {error_msg}")
            # Marquer toutes les √©tapes √† partir de l'√©tape actuelle (ou 1 si erreur tr√®s t√¥t) comme √©chou√©es
            start_fail_step = current_step if current_step > 0 else 1
            await self._mark_steps_as_status(url_index, start_fail_step, f"Critique: {error_msg}", ProgressStatus.ERROR)

        # Retourner la liste des fichiers cr√©√©s pour cette URL (peut √™tre vide si erreur)
        return all_files_for_this_url

    async def _process_transcripts_concurrently(self, videos: List[Video], url_index: int, step_number: int) -> Tuple[List[Video], int]:
        """R√©cup√®re les transcriptions de mani√®re concurrente pour une liste de vid√©os et met √† jour la progression."""
        total_v = len(videos)
        if total_v == 0: return [], 0 # Rien √† faire

        # Cr√©er les t√¢ches coroutines pour r√©cup√©rer chaque transcription
        coroutines = [
            self.transcript_manager.get_transcript(
                video.id, video.default_language, video.default_audio_language
            ) for video in videos
        ]

        logger.debug(f"Lancement de {len(coroutines)} t√¢ches de transcription concurrentes pour URL {url_index}")
        # Ex√©cuter les t√¢ches avec asyncio.gather, r√©cup√©rer r√©sultats ou exceptions
        results: List[Union[Optional[Dict[str, str]], Exception]] = await asyncio.gather(*coroutines, return_exceptions=True)
        logger.debug(f"Re√ßu {len(results)} r√©sultats/exceptions de la r√©cup√©ration concurrente des transcriptions pour URL {url_index}")

        processed_vids: List[Video] = [] # Liste pour conserver les vid√©os apr√®s traitement
        found_count = 0 # Compteur de transcriptions trouv√©es
        failed_count = 0 # Compteur d'√©checs/absences
        tasks_completed = 0 # Compteur pour la progression

        # Traiter les r√©sultats de gather
        for i, result in enumerate(results):
            tasks_completed += 1
            video = videos[i] # Obtenir l'objet Video correspondant

            # Mettre √† jour l'affichage de la progression de mani√®re fluide
            percent = (tasks_completed / total_v) * 100
            message = f"Traitement transcription {tasks_completed}/{total_v}"
            # Appeler la mise √† jour de l'√©tape (ne bloque pas longtemps)
            await self.progress_display.update_step(url_index, step_number, ProgressStatus.IN_PROGRESS, percent, message)

            if isinstance(result, Exception):
                # Une exception a √©t√© retourn√©e par gather pour cette t√¢che
                logger.warning(f"La r√©cup√©ration asynchrone de transcription a √©chou√© pour {video.id} (via gather): {result}", exc_info=False)
                failed_count += 1
                # Conserver l'objet vid√©o m√™me si la transcription √©choue
                processed_vids.append(video)
            elif result and isinstance(result, dict) and result.get("transcript"):
                # Transcription r√©cup√©r√©e et format√©e avec succ√®s
                video.transcript = result # Attacher le dict transcript √† l'objet vid√©o
                video.video_transcript_language = result.get('language') # Stocker la langue r√©elle
                found_count += 1
                logger.log(SPAM_LEVEL_NUM, f"Transcription assign√©e pour {video.id} (Langue: {video.video_transcript_language})")
                processed_vids.append(video)
            else:
                # Le r√©sultat est None ou un dict vide (non trouv√© ou formatage √©chou√©)
                failed_count += 1
                logger.log(SPAM_LEVEL_NUM, f"Transcription non trouv√©e ou vide pour {video.id}")
                processed_vids.append(video) # Conserver la vid√©o

        logger.info(f"Traitement des transcriptions pour URL {url_index} termin√© : {found_count} trouv√©es, {failed_count} √©chou√©es/absentes sur {total_v}.")
        # Retourner la liste des vid√©os (certaines peuvent maintenant avoir une transcription) et le nombre trouv√©
        return processed_vids, found_count

    async def process_urls(self, urls: List[str], output_format: str, include_transcript: bool, include_description: bool) -> List[Tuple[Path, int, int]]:
        """
        Traite une liste d'URLs s√©quentiellement, orchestrant le scraping pour chacune.
        S'arr√™te si le quota API est d√©pass√©.

        Args:
            urls: La liste des URLs YouTube √† traiter.
            output_format: Le format de sortie souhait√© ('txt', 'md', 'yaml').
            include_transcript: Faut-il inclure les transcriptions ?
            include_description: Faut-il inclure les descriptions ?

        Returns:
            Une liste de tous les fichiers cr√©√©s pour toutes les URLs trait√©es avec succ√®s.
        """
        all_created_files: List[Tuple[Path, int, int]] = [] # Accumulateur pour tous les fichiers
        total_urls_to_process = len(urls)
        # D√©marrer l'affichage Live pour l'ensemble du processus
        await self.progress_display.start_processing(total_urls_to_process)

        # It√©rer sur chaque URL fournie
        for url_idx, url in enumerate(urls, 1):
            # Arr√™ter imm√©diatement si le quota a √©t√© atteint lors du traitement d'une URL pr√©c√©dente
            if self.quota_reached:
                logger.warning(f"Quota API atteint. Ignorer URL {url_idx}/{total_urls_to_process}: {url}")
                # Marquer les √©tapes comme ignor√©es si l'en-t√™te a d√©j√† √©t√© affich√© pour cet index
                # (√©vite d'ajouter de nouvelles lignes si l'erreur survient avant show_url_header)
                if url_idx <= self.progress_display.url_counter:
                     await self._mark_steps_as_status(url_idx, 1, "Quota API d√©pass√©", ProgressStatus.QUOTA)
                continue # Passer √† l'URL suivante (qui sera aussi ignor√©e)

            files_for_current_url: List[Tuple[Path, int, int]] = []
            try:
                # Traiter l'URL actuelle (g√®re ses propres erreurs internes, y compris QuotaExceededError)
                files_for_current_url = await self.process_url(
                    url, url_idx, output_format, include_transcript, include_description
                )
                # Ajouter les fichiers cr√©√©s pour cette URL √† la liste globale
                all_created_files.extend(files_for_current_url)
            except QuotaExceededError:
                # Devrait √™tre attrap√©e dans process_url, mais s√©curit√© ici.
                # Le drapeau self.quota_reached est d√©j√† mis √† True.
                logger.info(f"QuotaExceededError attrap√©e pour URL {url_idx}, arr√™t des URLs suivantes.")
            except Exception as e:
                # Attraper les erreurs critiques inattendues qui remonteraient de process_url
                logger.critical(f"Erreur critique non g√©r√©e pour URL {url_idx} ({url}): {e}", exc_info=True)
                await self.progress_display.show_error(f"Erreur Critique traitant {url}: {e}")
                # Marquer les √©tapes comme √©chou√©es si l'en-t√™te a √©t√© affich√©
                if url_idx <= self.progress_display.url_counter:
                    await self._mark_steps_as_status(url_idx, 1, f"Critique: {e}", ProgressStatus.ERROR)
                # Continuer avec l'URL suivante malgr√© l'erreur critique sur celle-ci ?
                # Pour l'instant, oui. On pourrait choisir de s'arr√™ter ici (break).
            finally:
                # Ajouter une petite pause entre chaque URL, m√™me si pas d'erreur de quota,
                # pour √™tre un peu plus respectueux envers l'API/syst√®me.
                if not self.quota_reached:
                    await asyncio.sleep(0.1)

        # Arr√™ter l'affichage Live apr√®s avoir trait√© toutes les URLs (ou arr√™t√© √† cause du quota)
        await self.progress_display.stop()
        # Retourner la liste compl√®te des fichiers cr√©√©s
        return all_created_files

# ==============================================================================
# SECTION 9 : Interface CLI et Point d'Entr√©e
# ==============================================================================

async def main_cli() -> int:
    """Fonction principale asynchrone pour l'application en ligne de commande."""
    console = Console(stderr=True) # Utiliser stderr pour l'interface utilisateur
    progress_display = ProgressDisplay() # Initialiser le gestionnaire d'affichage
    exit_code = 0 # Code de sortie par d√©faut (succ√®s)

    try:
        # Afficher l'en-t√™te du script
        progress_display.show_header()

        # --- V√©rification de la Configuration ---
        try:
            # V√©rifier la pr√©sence de la cl√© API
            if not config.API_KEY:
                raise ValueError("Cl√© API YouTube (YOUTUBE_API_KEY) non trouv√©e dans les variables d'environnement.")
            logger.info(f"Configuration charg√©e. Dossier de travail : {config.WORK_FOLDER}")
        except ValueError as config_err:
            # Afficher l'erreur de configuration et quitter
            console.print(f"\n[bold red]ERREUR DE CONFIGURATION:[/]\n{config_err}")
            return 1 # Code de sortie 1 pour erreur de config

        # --- Entr√©e Utilisateur ---
        # Obtenir le mode, la valeur d'entr√©e, le format de sortie et les options d'inclusion
        mode, input_value, output_format, include_transcript, include_description = progress_display.show_menu()
        # V√©rifier si une valeur d'entr√©e a √©t√© fournie
        if not input_value:
            console.print("\n[bold red]ERREUR : URL ou chemin de fichier manquant.[/]")
            return 1

        # --- Chargement des URLs ---
        urls: List[str] = []
        if mode == "1": # Mode URL unique
            urls = [input_value.strip()] if input_value.strip() else []
        else: # Mode fichier
            urls = read_urls_from_file(input_value)

        # Filtrer les cha√Ænes vides potentielles apr√®s strip()
        urls = [u for u in urls if u]
        # V√©rifier s'il y a des URLs valides √† traiter
        if not urls:
            input_source_desc = "l'entr√©e utilisateur" if mode == '1' else f"le fichier '{input_value}'"
            console.print(f"\n[bold red]ERREUR : Aucune URL valide trouv√©e depuis {input_source_desc}.[/]")
            return 1

        logger.info(f"D√©but du traitement asynchrone pour {len(urls)} URL(s). Format: {output_format.upper()}, Transcriptions: {include_transcript}, Descriptions: {include_description}")

        # --- Initialisation des Composants ---
        api_client = YouTubeAPIClient(config.API_KEY)
        transcript_manager = TranscriptManager()
        engine = YoutubeScraperEngine(api_client, transcript_manager, progress_display)

        # --- Traitement Principal ---
        # Lancer le traitement des URLs et r√©cup√©rer la liste des fichiers cr√©√©s
        all_created_files = await engine.process_urls(
            urls, output_format, include_transcript, include_description
        )

        # --- Sortie Finale ---
        # L'affichage Live est arr√™t√© √† la fin de engine.process_urls
        # Afficher le tableau r√©capitulatif des fichiers
        progress_display.show_file_table(all_created_files)

        # D√©terminer le code de sortie final
        if engine.quota_reached:
             console.print("\n[bold yellow]INFO : Le traitement a √©t√© interrompu car le quota API YouTube a √©t√© d√©pass√©.[/]")
             exit_code = 2 # Code de sortie sp√©cifique pour quota d√©pass√©
        else:
             # Si le quota n'a pas √©t√© atteint et qu'aucune exception critique n'a stopp√© le script,
             # consid√©rer comme un succ√®s (code 0). Des erreurs sur des URLs sp√©cifiques
             # auront √©t√© loggu√©es et affich√©es mais n'emp√™chent pas le succ√®s global.
             exit_code = 0

    except ValueError as ve:
         # Attraper les erreurs de configuration ou de validation non intercept√©es plus t√¥t
         logger.critical(f"Erreur de configuration ou de validation : {ve}")
         await progress_display.stop() # Assurer l'arr√™t de l'affichage
         console.print(f"\n[bold red]ERREUR : {ve}[/]")
         exit_code = 1
    except KeyboardInterrupt:
         # G√©rer l'interruption par l'utilisateur (Ctrl+C)
         logger.warning("Interruption utilisateur (Ctrl+C) d√©tect√©e.")
         await progress_display.stop()
         console.print("\n[yellow]Traitement interrompu par l'utilisateur.[/]")
         exit_code = 130 # Code de sortie standard pour Ctrl+C
    except Exception as e:
         # G√©rer les erreurs critiques inattendues dans la fonction principale
         logger.critical(f"Erreur critique inattendue dans main_cli : {e}", exc_info=True)
         await progress_display.stop()
         # Afficher un message d'erreur clair √† l'utilisateur
         console.print(f"\n[bold red]ERREUR CRITIQUE INATTENDUE:[/]\n{e}\nConsultez le fichier log : {config.WORK_FOLDER / 'youtube_scraper_async.log'}")
         exit_code = 1 # Code de sortie pour erreur g√©n√©rale
    finally:
        # Assurer que l'affichage Live est toujours arr√™t√©, m√™me en cas d'erreur pr√©coce
        await progress_display.stop()
        logger.info(f"Ex√©cution du script asynchrone termin√©e. Code de sortie final : {exit_code}")

    # Retourner le code de sortie d√©termin√©
    return exit_code

# Point d'entr√©e principal du script
if __name__ == "__main__":
    final_exit_code = 1 # Code de sortie par d√©faut en cas d'erreur tr√®s pr√©coce

    # Bloc try/except global pour lancer la fonction asynchrone principale
    try:
        # Ex√©cuter la coroutine main_cli et r√©cup√©rer son code de sortie
        final_exit_code = asyncio.run(main_cli())
    except KeyboardInterrupt:
        # Attraper Ctrl+C si cela se produit avant/pendant le d√©marrage de la boucle asyncio
        print("\n[yellow]Script interrompu pendant le d√©marrage ou l'arr√™t.[/]")
        final_exit_code = 130
    except Exception as main_exception:
        # Attraper les erreurs fatales lors du d√©marrage d'asyncio.run
        print(f"\n[bold red]ERREUR FATALE AU D√âMARRAGE:[/]\n{main_exception}")
        # Essayer de logger l'erreur si possible
        try:
            logger.critical(f"Erreur fatale dans le bloc __main__ : {main_exception}", exc_info=True)
        except:
            pass # Ignorer si le logging lui-m√™me √©choue
        final_exit_code = 1
    finally:
        # Quitter le script avec le code de sortie final d√©termin√©
        sys.exit(final_exit_code)
